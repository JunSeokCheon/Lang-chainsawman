{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPBTZdYYKp9SyuSuc25y46"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"67FyPjMZRdSj"},"outputs":[],"source":["import os\n","os.environ['OPENAI_API_KEY'] = \"EXAMPLE\""]},{"cell_type":"markdown","source":["# LCEL 다루는 방법"],"metadata":{"id":"w9jImEjc4wvx"}},{"cell_type":"markdown","source":["# RunnableParallel : 입력 및 출력 조작\n","##### RunnableParallel은 시퀀스에서 다음 Runnable 입력 형식과 일치하도록 하나의 Runnable의 출력을 조작한다."],"metadata":{"id":"RXA-niB9QwhM"}},{"cell_type":"code","source":["!pip install --upgrade --quiet  langchain langchain-openai"],"metadata":{"id":"9eblZatXTKym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install faiss-gpu"],"metadata":{"id":"7ZZ2x3__UaGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","\n","vectorstore = FAISS.from_texts(\n","    ['천준석은 인천에서 일을 한다.'], embedding=OpenAIEmbeddings()\n",")\n","retriever = vectorstore.as_retriever()\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","model = ChatOpenAI()\n","\n","retrieval_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt | model | StrOutputParser()\n",")\n","\n","retrieval_chain.invoke(\"천준석은 어디서 일하니?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wlwlUKp6QvvN","executionInfo":{"status":"ok","timestamp":1710996208984,"user_tz":-540,"elapsed":1741,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"1f82ebd6-04bd-490f-88c4-b88175ad6a4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'인천'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## 약어로 itemgetter 사용\n","##### RunnableParallel와 결합하여 데이터를 추출하기 위해 python 약어를 사용할 수 있다.\n","##### itemgetter를 사용하여 맵에서 특정 키를 추출한다."],"metadata":{"id":"G7e8C0BaVHvV"}},{"cell_type":"code","source":["from operator import itemgetter\n","\n","from langchain_community.vectorstores import FAISS\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","\n","vectorstore = FAISS.from_texts(\n","    [\"천준석은 인천에서 일을 한다.\"], embedding=OpenAIEmbeddings()\n",")\n","retriever = vectorstore.as_retriever()\n","\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\n","Answer in the following language: {language}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","chain = (\n","    {\n","        \"context\": itemgetter(\"question\") | retriever,\n","        \"question\": itemgetter(\"question\"),\n","        \"language\": itemgetter(\"language\"),\n","    }\n","    | prompt | model | StrOutputParser()\n",")\n","\n","chain.invoke({\"question\": \"천준석은 어디서 일하니?\", \"language\": \"english\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"SX9At2XiULvo","executionInfo":{"status":"ok","timestamp":1710997592015,"user_tz":-540,"elapsed":1446,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"597f1dfa-7008-4967-9e15-14eeb32d9f99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Cheon Jun-seok works in Incheon.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## 병렬화\n","##### RunnableParallel 을 사용하면 여러 Runnable을 병렬로 쉽게 실행하고 이러한 Runnable의 출력을 반환할 수 있다."],"metadata":{"id":"v5lo-CGAZ9Wc"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnableParallel\n","from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI()\n","joke_chain = ChatPromptTemplate.from_template(\"{주제}에 대해서 농담을 말해줘\") | model\n","poem_chain = (\n","    ChatPromptTemplate.from_template(\"{주제}에 대해서 2줄 정도의 시를 써줘\") | model\n",")\n","\n","map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n","\n","map_chain.invoke({\"주제\":\"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQJIUMBIZ03f","executionInfo":{"status":"ok","timestamp":1710997811091,"user_tz":-540,"elapsed":1443,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a1772b81-3326-4a80-e10b-e4cd61cc76ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'joke': AIMessage(content='곰이 왜 항상 참견을 하는가?\\n\\n곰곰히 생각하면 당연하다!', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 23, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None}),\n"," 'poem': AIMessage(content='숲속 깊은 곳에서 뚜벅뚜벅 걸어오는\\n귀여운 곰이 나를 보며 웃는다.', response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 26, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["%%timeit\n","\n","joke_chain.invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z0wmPeZaqf-","executionInfo":{"status":"ok","timestamp":1710997869126,"user_tz":-540,"elapsed":10783,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"720ee4e7-7c14-44e6-acbf-98de8068f6b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.37 s ± 358 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"code","source":["%%timeit\n","\n","poem_chain.invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LN9oRSa6a2Qw","executionInfo":{"status":"ok","timestamp":1710997901988,"user_tz":-540,"elapsed":9995,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"96c50d8d-4b0d-4ccf-dd1a-7ab9bae134f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.24 s ± 335 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"code","source":["%%timeit\n","\n","map_chain.invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye9sd8Uda-hE","executionInfo":{"status":"ok","timestamp":1710997935409,"user_tz":-540,"elapsed":11673,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"40ebb321-ccdc-47dc-ada0-e7813cc4073a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.49 s ± 406 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"markdown","source":["##### 두 개를 동시 실행 한다해도 동일하거나 더 빠른 런타임을 갖는 것을 볼 수 있다."],"metadata":{"id":"DQrhHqsjbHIQ"}},{"cell_type":"markdown","source":["# RunnablePassthrough : 데이터 전달\n","##### RunnablePassthrough를 사용하면 변경 없이 또는 추가 키를 추가하여 입력을 전달할 수 있다. 일반적으로 RunnableParallel과 함께 사용되어 맵의 새 키에 데이터를 할당한다.\n","##### 자체적으로 호출되는 RunnablePassthrough()는 입력을 받아 통과시킨다.\n","##### RunnablePassthrough는 RunnablePassthrough.assign 으로 호출된 입력을 취하고 할당 함수에 전달된 추가 인수를 추가한다."],"metadata":{"id":"wu0gT9k_bWbJ"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","\n","runnable = RunnableParallel(\n","    passed=RunnablePassthrough(),\n","    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n","    modified=lambda x: x[\"num\"] + 1,\n",")\n","\n","runnable.invoke({\"num\": 1})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DU5wq8nlbEor","executionInfo":{"status":"ok","timestamp":1710998543952,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"52306abe-79a9-4279-929d-4914ed7c95b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## 검색\n","##### RunnableMap과 RunnablePassthrough를 함께 사용하는 예제는 보자"],"metadata":{"id":"gy6a42lbdksZ"}},{"cell_type":"code","source":["from langchain_community.vectorstores import FAISS\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings"],"metadata":{"id":"fM1v6gC-dbgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorstore = FAISS.from_texts(\n","    [\"천준석은 김치를 먹는다.\"], embedding=OpenAIEmbeddings()\n",")\n","retriever = vectorstore.as_retriever()\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","model = ChatOpenAI()\n","\n","retrieval_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")\n","\n","retrieval_chain.invoke(\"천준석은 뭐를 먹니?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"H_C-Wd-KePQc","executionInfo":{"status":"ok","timestamp":1710998768368,"user_tz":-540,"elapsed":1201,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"5f0c2216-fb1c-40d1-9c43-415e824cb103"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'김치'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# RunnableLambda : 사용자 정의 함수 실행\n","##### 이러한 함수는 모든 입력이 SINGLE 인수여여 하고, 여러 인수를 허용하는 함수가 있는 경우 단일 입력을 허용하고 압축 해제하는 래퍼를 작성해야 한다."],"metadata":{"id":"-7zBF3NIeWWD"}},{"cell_type":"code","source":["from operator import itemgetter\n","\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnableLambda\n","from langchain_openai import ChatOpenAI\n","\n","\n","def length_function(text):\n","    return len(text)\n","\n","\n","def _multiple_length_function(text1, text2):\n","    return len(text1) * len(text2)\n","\n","\n","def multiple_length_function(_dict):\n","    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n","\n","\n","prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n","model = ChatOpenAI()\n","\n","chain1 = prompt | model\n","\n","chain = (\n","    {\n","        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n","        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n","        | RunnableLambda(multiple_length_function),\n","    }\n","    | prompt\n","    | model\n",")"],"metadata":{"id":"-wPQSkZReUN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvK_Y4Dofa_A","executionInfo":{"status":"ok","timestamp":1710999063180,"user_tz":-540,"elapsed":402,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"981f816e-75c9-41cb-f2b6-e089681569da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='3 + 9 = 12', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 실행 가능한 내용\n","##### 실행 가능한 람다는 콜백, 태그 및 기타 구성 정보를 중천된 실행에 전달하는데 사용할 수 있는 RunnableConfig를 선택적으로 허용한다."],"metadata":{"id":"agI6uC8afrDK"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableConfig"],"metadata":{"id":"7GqORgWKfcYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","\n","def parse_or_fix(text: str, config: RunnableConfig):\n","    fixing_chain = (\n","        ChatPromptTemplate.from_template(\n","            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n","            \" Don't narrate, just respond with the fixed data.\"\n","        )\n","        | ChatOpenAI()\n","        | StrOutputParser()\n","    )\n","    for _ in range(3):\n","        try:\n","            return json.loads(text)\n","        except Exception as e:\n","            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n","    return \"Failed to parse\""],"metadata":{"id":"C0huonkTgBEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.callbacks import get_openai_callback\n","\n","with get_openai_callback() as cb:\n","    output = RunnableLambda(parse_or_fix).invoke(\n","        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n","    )\n","    print(output)\n","    print(cb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vJ4P4VZgD4V","executionInfo":{"status":"ok","timestamp":1710999229936,"user_tz":-540,"elapsed":698,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a4e2510c-c9bf-4468-d3ae-2e49f98bf703"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'foo': 'bar'}\n","Tokens Used: 62\n","\tPrompt Tokens: 56\n","\tCompletion Tokens: 6\n","Successful Requests: 1\n","Total Cost (USD): $9.6e-05\n"]}]},{"cell_type":"markdown","source":["# RunnableBranch : 입력을 기반으로 로직을 동적으로 라우팅\n","##### 라우팅을 사용하면 LLM과의 상호 작용에 대한 구조와 일관성을 제공한다.\n","##### 수행 방법으로는 2가지가 있다.\n","\n","\n","1.   RunnableLambda : 조건부로 실행 파일 반환 (선호)\n","2.  RunnableBranch\n","\n"],"metadata":{"id":"c_MTx0H9gZoM"}},{"cell_type":"markdown","source":["##### 먼저 들어오는 질문을 langchain, openai, other 로 구분하여 식별하는 체인을 만들어보자."],"metadata":{"id":"cBXIM06giIQf"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import PromptTemplate\n","\n","chain = (\n","    PromptTemplate.from_template(\n","        \"\"\"Given the user question below, classify it as either being about `LangChain`, `openai`, or `Other`.\n","\n","Do not respond with more than one word.\n","\n","<question>\n","{question}\n","</question>\n","\n","Classification:\"\"\"\n","    )\n","    | ChatOpenAI()\n","    | StrOutputParser()\n",")\n","\n","chain.invoke({\"question\": \"how do I call openai?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KI8QoXjdgFB6","executionInfo":{"status":"ok","timestamp":1711000088074,"user_tz":-540,"elapsed":415,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"dd2cbba7-f9be-4659-8842-35e21e7bf871"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'openai'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["langchain_chain = (\n","    PromptTemplate.from_template(\n","        \"\"\"You are an expert in langchain. \\\n","Always answer questions starting with \"As Harrison Chase told me\". \\\n","Respond to the following question:\n","\n","Question: {question}\n","Answer:\"\"\"\n","    )\n","    | ChatOpenAI()\n",")\n","openai_chain = (\n","    PromptTemplate.from_template(\n","        \"\"\"You are an expert in openai. \\\n","Always answer questions starting with \"As Dario Amodei told me\". \\\n","Respond to the following question:\n","\n","Question: {question}\n","Answer:\"\"\"\n","    )\n","    | ChatOpenAI()\n",")\n","general_chain = (\n","    PromptTemplate.from_template(\n","        \"\"\"Respond to the following question:\n","\n","Question: {question}\n","Answer:\"\"\"\n","    )\n","    | ChatOpenAI()\n",")"],"metadata":{"id":"UhMy9ZNljU4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 사용자 정의 기능을 사용하여 서로 다른 출력 간의 라우팅을 해보자"],"metadata":{"id":"ZM1eB_q3jepM"}},{"cell_type":"code","source":["def route(info):\n","    if \"openai\" in info[\"topic\"].lower():\n","        return openai_chain\n","    elif \"langchain\" in info[\"topic\"].lower():\n","        return langchain_chain\n","    else:\n","        return general_chain"],"metadata":{"id":"Itala6qJjc0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","\n","full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n","    route\n",")"],"metadata":{"id":"dYavl711jk3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_chain.invoke({\"question\": \"how do I use openai?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xge473bZjoRz","executionInfo":{"status":"ok","timestamp":1711000171691,"user_tz":-540,"elapsed":1671,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"979cdae8-e386-40c3-962e-3b19d6c1f2b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='As Dario Amodei told me, you can use OpenAI by signing up for an API key on their website and following the documentation to integrate their models into your applications.', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 47, 'total_tokens': 83}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["full_chain.invoke({\"question\": \"how do I use LangChain?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNzIKk91jquL","executionInfo":{"status":"ok","timestamp":1711000187139,"user_tz":-540,"elapsed":2102,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d8d88e1d-981a-4e00-9529-f0f792e71a08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='As Harrison Chase told me, to use LangChain, you need to first create an account on the platform and then start exploring the various language learning features it offers. You can practice speaking, listening, reading, and writing in your target language, and track your progress along the way. Additionally, you can connect with language tutors and other learners to further enhance your language skills.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 44, 'total_tokens': 119}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["full_chain.invoke({\"question\": \"whats 2 + 2\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyzh2NQ3juWm","executionInfo":{"status":"ok","timestamp":1711000191027,"user_tz":-540,"elapsed":1029,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"9b769eea-d71d-469a-8098-5a59638a6cd7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='2 + 2 equals 4.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## RunnableBranch 사용\n","##### 입력을 기반으로 실행할 조건 및 실행 가능 항목 세트를 정의할 수 있는 특수한 유형의 방법입니다. 그런데, 사용자 정의 함수에서 모두 제공하기에 사용자 정의 함수를 쓰는 것을 선호됩니다."],"metadata":{"id":"aHeZr1TZj4RH"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableBranch\n","\n","branch = RunnableBranch(\n","    (lambda x: \"openai\" in x[\"topic\"].lower(), openai_chain),\n","    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n","    general_chain,\n",")\n","full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n","full_chain.invoke({\"question\": \"how do I use openai?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCUXoTNGjvls","executionInfo":{"status":"ok","timestamp":1711003179714,"user_tz":-540,"elapsed":2778,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"108babfa-8d67-4ac9-bd63-4395ee9602ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"As Dario Amodei told me, to use OpenAI, you can start by signing up for an API key on their website. This key will allow you to access their various language models and tools for natural language processing and generation. You can then use this API key to integrate OpenAI's technology into your own applications or projects.\", response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 47, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["full_chain.invoke({\"question\": \"how do I use LangChain?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M1_vIoxKkLsn","executionInfo":{"status":"ok","timestamp":1711003130394,"user_tz":-540,"elapsed":2968,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"2e877ee4-4094-49ea-f5e4-b572a259d258"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='As Harrison Chase told me, to use LangChain, you must first create an account on the platform and then begin by selecting the language you wish to learn. From there, you can access a variety of language learning tools, including lessons, exercises, and conversation practice. It is important to regularly practice and engage with the material to effectively improve your language skills.', response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 44, 'total_tokens': 116}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["full_chain.invoke({\"question\": \"whats 2 + 2\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cn7Fj9dtu8us","executionInfo":{"status":"ok","timestamp":1711003139676,"user_tz":-540,"elapsed":1431,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f98480de-8121-44a1-ea3f-7fa87b8d73f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='4', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 24, 'total_tokens': 25}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["# 런타임 인수 바인딩\n","##### Runnable.bind()를 사용해서 상수 인수를 사용하여 Runnable 시퀀스 내에서 Runnable을 호출할 수 있다."],"metadata":{"id":"sUq2pCVPvbmT"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"1onpRJ1Zu_Tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n","        ),\n","        (\"human\", \"{equation_statement}\"),\n","    ]\n",")\n","\n","model = ChatOpenAI(temperature=0)\n","runnable = (\n","    {\n","        \"equation_statement\": RunnablePassthrough()\n","    }\n","    | prompt | model | StrOutputParser()\n",")\n","\n","print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fD8oA4jqvz0t","executionInfo":{"status":"ok","timestamp":1711003709773,"user_tz":-540,"elapsed":1188,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"615a7b33-949f-4ad2-c98b-3216a407fa33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EQUATION: x^3 + 7 = 12\n","\n","SOLUTION: \n","Subtract 7 from both sides:\n","x^3 = 5\n","\n","Take the cube root of both sides:\n","x = ∛5\n"]}]},{"cell_type":"markdown","source":["##### stop 단어로 모델을 호출할 수 있다."],"metadata":{"id":"LR0WFxp7xNiD"}},{"cell_type":"code","source":["runnable = (\n","    {\"equation_statement\" : RunnablePassthrough()}\n","    | prompt | model.bind(stop=\"SOLUTION\") | StrOutputParser()\n",")\n","\n","print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"om4kKMQnw_1x","executionInfo":{"status":"ok","timestamp":1711003789097,"user_tz":-540,"elapsed":1145,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"bc9c1221-bd6c-452e-e504-2ead51d67420"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EQUATION: x^3 + 7 = 12\n","\n","\n"]}]},{"cell_type":"markdown","source":["## OpenAI\n","##### 바인딩의 특히 응용 프로그램 중 하나인 OpenAI 기능을 호환되는 OpenAI 모델에 연결하는 것이다."],"metadata":{"id":"O1FxFGRXxkST"}},{"cell_type":"code","source":["tools = [\n","    {\n","        \"type\": \"function\",\n","        \"function\": {\n","            \"name\": \"get_current_weather\",\n","            \"description\": \"Get the current weather in a given location\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"location\": {\n","                        \"type\": \"string\",\n","                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n","                    },\n","                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n","                },\n","                \"required\": [\"location\"],\n","            },\n","        },\n","    }\n","]"],"metadata":{"id":"Zvt-bnVtxd_l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ChatOpenAI(model=\"gpt-3.5-turbo\").bind(tools=tools)\n","model.invoke(\"Waht's the weather in South Korea, Incheon\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zrf39R4Qxw45","executionInfo":{"status":"ok","timestamp":1711003906946,"user_tz":-540,"elapsed":1236,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"5e9b9697-1a53-49be-b9b7-51e018be0680"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_CVCxg3bhs1DU9IpF96sMh377', 'function': {'arguments': '{\"location\":\"Incheon, South Korea\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 85, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'tool_calls', 'logprobs': None})"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["# 런타임에 체인 내부 구성\n","##### 작업을 수행하는 여러 가지 다른 방법을 실험하거나 사용자에게 노출시키는 방법이 있다.\n","\n","\n","1.   configurable_fields : 실행 가능 항목의 특정 필드를 구성한다.\n","2.   configurable_alternatives : 런타임 중에 설정할 수 있는 특정 실행 가능 항목 대안을 나열한다.\n","\n"],"metadata":{"id":"X112_BEIyR7V"}},{"cell_type":"markdown","source":["## configurable_fields\n","### LLM\n","##### LLM을 사용하면 온도와 같은 항목을 구성할 수 있다."],"metadata":{"id":"EQi3tjyqy8RW"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_core.runnables import ConfigurableField\n","from langchain_openai import ChatOpenAI\n","\n","model = ChatOpenAI(temperature=0).configurable_fields(\n","    temperature=ConfigurableField(\n","        id=\"llm_temperature\",\n","        name=\"LLM Temperature\",\n","        description=\"The temperature of the LLM\"\n","    )\n",")"],"metadata":{"id":"AYm6FBJyx6wG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.invoke(\"숫자 하나만 무작위로 말해바~\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYj9dvBPzSyq","executionInfo":{"status":"ok","timestamp":1711004294135,"user_tz":-540,"elapsed":846,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a4e247ab-53f6-4d0d-a618-5e38f7e921ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='7', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 24, 'total_tokens': 25}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"숫자 하나만 무작위로 말해바~\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24hvwXbazZYG","executionInfo":{"status":"ok","timestamp":1711004340401,"user_tz":-540,"elapsed":935,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d8094606-404f-44c7-8555-970e8338c520"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='5', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 24, 'total_tokens': 25}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["##### 체인을 일부로 해당 작업을 수행할 수 있다."],"metadata":{"id":"IJe1VC_iznEN"}},{"cell_type":"code","source":["prompt = PromptTemplate.from_template(\"{x}보다 큰 숫자 하나만 골라바\")\n","chain = prompt | model"],"metadata":{"id":"d50RpeK3zjcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"x\": 15})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFglmceSzyXz","executionInfo":{"status":"ok","timestamp":1711004441571,"user_tz":-540,"elapsed":838,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"293cfdb4-ac8c-42aa-9e25-ecd58bf0061d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='7', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 18, 'total_tokens': 19}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 7})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYurPN03z0U9","executionInfo":{"status":"ok","timestamp":1711004504910,"user_tz":-540,"elapsed":466,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"ba8d6c89-3265-4713-b292-f812882ca20e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=' 3', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 18, 'total_tokens': 20}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["## HubRunnables 사용\n","##### 프롬프트 전환에 유용하다."],"metadata":{"id":"jDZCROg20ZWN"}},{"cell_type":"code","source":["!pip install langchainhub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZGRhHow0vrd","executionInfo":{"status":"ok","timestamp":1711004653620,"user_tz":-540,"elapsed":5000,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"859502aa-8d15-4147-f142-5c518ed6ff18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchainhub\n","  Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n","Installing collected packages: types-requests, langchainhub\n","Successfully installed langchainhub-0.1.15 types-requests-2.31.0.20240311\n"]}]},{"cell_type":"code","source":["from langchain.runnables.hub import HubRunnable"],"metadata":{"id":"qlnZdV6p0KO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n","    owner_repo_commit=ConfigurableField(\n","        id=\"hub_commit\",\n","        name=\"Hub Commit\",\n","        description=\"The Hub commit to pull from\",\n","    )\n",")"],"metadata":{"id":"3xwEVHv30fqh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiS3-YDn0tWj","executionInfo":{"status":"ok","timestamp":1711004707739,"user_tz":-540,"elapsed":331,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f924f022-51fa-4e88-e795-beed41fcfb36"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:\")])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["prompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke(\n","    {\"question\": \"foo\", \"context\": \"bar\"}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POvK0oDE0-hs","executionInfo":{"status":"ok","timestamp":1711004743974,"user_tz":-540,"elapsed":439,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"569033ec-d216-429d-f436-1cb83efdc7ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]\")])"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["## configurable_alternatives\n","### LLM"],"metadata":{"id":"FiX575tM1L0U"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain_core.runnables import ConfigurableField\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"URZmATJn1HPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0).configurable_alternatives(\n","    ConfigurableField(id=\"llm\"),\n","    default_key=\"anthropic\",\n","    openai=ChatOpenAI(),\n",")\n","prompt = PromptTemplate.from_template(\"{주제}에 대해서 농담 하나 말해바\")\n","chain = prompt | llm"],"metadata":{"id":"ZMoQX4Ye1exz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"주제\":\"인천\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrDYWN6w1--G","executionInfo":{"status":"ok","timestamp":1711005105937,"user_tz":-540,"elapsed":1945,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a31d2d37-c86e-40dc-fa83-8d2e1f980eb9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='인천 사람들은 바다가 가까워서 물 한 모금 마시면 다 바다 물이 된다던데요!', response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 25, 'total_tokens': 68}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QH7VfEgl2CO3","executionInfo":{"status":"ok","timestamp":1711005175752,"user_tz":-540,"elapsed":1764,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d6ee51ea-e288-45aa-92b8-18223d822750"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='곰이 어떤 차에서 가장 잘 탈까요? \\n\\n포르쉐! 왜냐하면 포르쉐 ㅋㅋㅋ', response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 24, 'total_tokens': 75}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["## 프롬프트\n","##### 프롬프트를 번갈아 가며 수행할 수 있다."],"metadata":{"id":"I7LQmlVk3KZ5"}},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0)\n","prompt = PromptTemplate.from_template(\n","    \"{주제}에 대해서 농담 해바\"\n",").configurable_alternatives(\n","    ConfigurableField(id=\"prompt\"),\n","    default_key=\"joke\",\n","    poem=PromptTemplate.from_template(\"{주제}에 대해서 짧은 시를 써줘\"),\n",")\n","chain = prompt | llm"],"metadata":{"id":"he5RPw7f2wdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmvzIV3D3WD0","executionInfo":{"status":"ok","timestamp":1711005357321,"user_tz":-540,"elapsed":1874,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"82d84a68-2175-4f94-e141-7ccda461c14b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='왜 곰이 항상 피곤한가요? \\n\\n- 당연히 자다가 깨어나면서 \"곰이다\" 하면서 피곤해요!', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 20, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaQjXQ063cww","executionInfo":{"status":"ok","timestamp":1711005405583,"user_tz":-540,"elapsed":1619,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"3cf10921-57b4-4a0f-fd10-188d25deb7c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='숲 속 깊은 곳에\\n턱시도 높은 곰이 산다\\n턱시도 높은 곰이\\n숲 속 깊은 곳에 산다', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 22, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["## 프롬프트와 LLM 구성"],"metadata":{"id":"K3w1C_Hh3xQr"}},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0).configurable_alternatives(\n","    ConfigurableField(id=\"llm\"),\n","    default_key=\"anthropic\",\n","    openai=ChatOpenAI(),\n",")\n","prompt = PromptTemplate.from_template(\n","    \"{주제}에 대해서 농담 해바\"\n",").configurable_alternatives(\n","    ConfigurableField(id=\"prompt\"),\n","    default_key=\"joke\",\n","    poem=PromptTemplate.from_template(\"{주제}에 대해서 짧은 시를 써줘\"),\n",")\n","chain = prompt | llm"],"metadata":{"id":"s62CfmR13odd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(\n","    {\"주제\": \"곰\"}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0pSg93E33ff","executionInfo":{"status":"ok","timestamp":1711005577331,"user_tz":-540,"elapsed":4570,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f54199ce-02f8-4b9f-c1d4-2299103b9cc3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='숲 속 깊은 곳에서\\n큼지막한 곰이 살아요\\n털부터 발끝까지 검은색\\n둥실둥실한 몸집에\\n우쭈쭈한 뒷발로\\n뛰어놀며 놀아요\\n\\n숲속을 터벅터벅\\n달려다니는 모습은\\n아름다운 자연의 힘\\n곰이라는 친구가\\n우리 옆에서 함께\\n살아가는 모습을 보면\\n우린 행복을 느껴요.', response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 22, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"주제\": \"곰\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d0Do-i_345D","executionInfo":{"status":"ok","timestamp":1711005580965,"user_tz":-540,"elapsed":1286,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"6c78c92c-70e5-4e96-fc35-cd7230a9f789"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='왜 곰은 항상 화내나요? \\n\\n- 왜냐하면 항상 곰곰이 생각하니까요!', response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 20, 'total_tokens': 60}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":75}]},{"cell_type":"markdown","source":["# @chain 데코레이터를 사용하여 실해 가능 파일 만들기\n","##### @chain 데코레이터를 추가하여 임의의 함수를 체인으로 바꿀수있다."],"metadata":{"id":"MGPY9K9o4CSq"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import chain\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"YTgaO1oH37aZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt1 = ChatPromptTemplate.from_template(\"{주제}에 대해서 농담 해줘\")\n","prompt2 = ChatPromptTemplate.from_template(\"{농담} 해당 농담에 대한 주제가 뭐야\")"],"metadata":{"id":"9TMRFwoJ5M8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@chain\n","def custom_chain(text):\n","    prompt_val1 = prompt1.invoke({\"주제\": text})\n","    output1 = ChatOpenAI().invoke(prompt_val1)\n","    parsed_output1 = StrOutputParser().invoke(output1)\n","    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n","    return chain2.invoke({\"농담\": parsed_output1})"],"metadata":{"id":"gyoJqxOh5hiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_chain.invoke(\"bears\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"86qDlXNn5iXS","executionInfo":{"status":"ok","timestamp":1711005921884,"user_tz":-540,"elapsed":2259,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"125b80b2-e5c2-4bb7-e70b-5e07c087ac38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'곰 같이 친절한 사람들에 대한 농담입니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["# 폴백 추가\n","##### LLM API 문제, 잘못된 모델 출력, 기타 통합 문제 등 LLM 애플리케이션에는 실패할 수 있는 지점이 많기 때문에 폴백을 적절하게 사용하면 도움이 많이 된다."],"metadata":{"id":"QsUCRTj75wRJ"}},{"cell_type":"markdown","source":["## LLM API\n","##### LLM API 요청으로 인해 실패를 폴백을 사용하면 보호하는데 도움이 된다."],"metadata":{"id":"cWxEHB3W6JfS"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_community.chat_models import ChatAnthropic"],"metadata":{"id":"m27e79MZ5jFM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### OpenAI에서 폴백을 포함한 LLM이 아래의 예이다"],"metadata":{"id":"8RMSqDTL6p1X"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You're a nice assistant who always includes a compliment in your response\",\n","        ),\n","        (\"human\", \"Why did the {animal} cross the road\"),\n","    ]\n",")\n","chain = prompt | llm\n","with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n","    try:\n","        print(chain.invoke({\"animal\": \"kangaroo\"}))\n","    except RateLimitError:\n","        print(\"Hit error\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oPFxhtI6oM6","executionInfo":{"status":"ok","timestamp":1711006380453,"user_tz":-540,"elapsed":431,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"78cbf84f-8189-4098-e7e9-be10d7a3d685"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hit error\n"]}]},{"cell_type":"markdown","source":["# 사용자 정의 생성기 함수 스트림\n","##### LCEL 파이프라인에서 yield 생성기 함수를 사용할 수 있다.\n","##### 이러한 생성기의 서명은 Iterator[Input] -> Iterator[Output], 비동기 생성기의 경우 AsyncIterator[Input] -> AsyncIterator[Output]\n","##### 사용자 정의 출력 파서를 구현하거나 스트리밍 기능을 유지하면서 이전 단계의 출력을 수정할 때 용이하다."],"metadata":{"id":"KBsNnlZW7qRw"}},{"cell_type":"markdown","source":["## 동기"],"metadata":{"id":"qWvffoyW8WYI"}},{"cell_type":"code","source":["from typing import Iterator, List\n","\n","from langchain.prompts.chat import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","prompt = ChatPromptTemplate.from_template(\n","    \"Write a comma-separated list of 5 animals similar to: {animal}\"\n",")\n","model = ChatOpenAI(temperature=0.0)\n","\n","str_chain = prompt | model | StrOutputParser()"],"metadata":{"id":"f6Nq40nQ62es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in str_chain.stream({\"animal\": \"bear\"}):\n","  print(chunk, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJ1gJe4p64XK","executionInfo":{"status":"ok","timestamp":1711006689252,"user_tz":-540,"elapsed":1534,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"572db0f4-6180-4875-92fa-b2cd4e7ca5d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Wolf\n","2. Lion\n","3. Tiger\n","4. Gorilla\n","5. Panda"]}]},{"cell_type":"code","source":["str_chain.invoke({\"animal\": \"bear\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KORoyoj38iBe","executionInfo":{"status":"ok","timestamp":1711006734515,"user_tz":-540,"elapsed":933,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"80e4f3b1-0a07-4b2b-ade5-e34eff7df359"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1. Wolf\\n2. Tiger\\n3. Lion\\n4. Gorilla\\n5. Panda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n","    buffer = \"\"\n","    for chunk in input:\n","        buffer += chunk\n","        while \",\" in buffer:\n","            comma_index = buffer.index(\",\")\n","            yield [buffer[:comma_index].strip()]\n","            buffer = buffer[comma_index + 1 :]\n","    yield [buffer.strip()]"],"metadata":{"id":"8TSoMLTQ8tNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_chain = str_chain | split_into_list"],"metadata":{"id":"aBfzKus582U7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in list_chain.stream({\"animal\": \"bear\"}):\n","  print(chunk, flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSOUM6ll89JE","executionInfo":{"status":"ok","timestamp":1711006819967,"user_tz":-540,"elapsed":1179,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"0afa89a9-cdfa-4a6d-9baa-5e691945c168"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['1. Wolf']\n","['2. Lion']\n","['3. Tiger']\n","['4. Gorilla']\n","['5. Panda']\n"]}]},{"cell_type":"markdown","source":["## 비동기"],"metadata":{"id":"Jdshrdpv9DRK"}},{"cell_type":"code","source":["from typing import AsyncIterator\n","\n","\n","async def asplit_into_list(\n","    input: AsyncIterator[str],\n",") -> AsyncIterator[List[str]]:  # async def\n","    buffer = \"\"\n","    async for (\n","        chunk\n","    ) in input:  # `input` is a `async_generator` object, so use `async for`\n","        buffer += chunk\n","        while \",\" in buffer:\n","            comma_index = buffer.index(\",\")\n","            yield [buffer[:comma_index].strip()]\n","            buffer = buffer[comma_index + 1 :]\n","    yield [buffer.strip()]\n","\n","\n","list_chain = str_chain | asplit_into_list"],"metadata":{"id":"WwW13hUz9B9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async for chunk in list_chain.astream({\"animal\": \"bear\"}):\n","    print(chunk, flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xk4aSYH9MlU","executionInfo":{"status":"ok","timestamp":1711006866228,"user_tz":-540,"elapsed":853,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f659c0cf-bed9-4d93-c4ac-fc9f40d36824"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['1. Wolf\\n2. Tiger\\n3. Lion\\n4. Gorilla\\n5. Panda']\n"]}]},{"cell_type":"code","source":["await list_chain.ainvoke({\"animal\": \"bear\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQ6wJM6X9NP9","executionInfo":{"status":"ok","timestamp":1711006873239,"user_tz":-540,"elapsed":809,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"09254ffc-621e-4c9c-d6d6-f3d90f399d1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1. Wolf\\n2. Lion\\n3. Tiger\\n4. Gorilla\\n5. Panda']"]},"metadata":{},"execution_count":96}]},{"cell_type":"markdown","source":["# 실행 파일 검사\n","##### LCEL을 사용하여 실행 가능 파일을 생성한 후에는 무슨 일이 일어나고 있는지 검사할 수 있다."],"metadata":{"id":"I8TkXR-j9RbD"}},{"cell_type":"code","source":["!pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"],"metadata":{"id":"3tbx_D9T9O-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install grandalf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmf3cplE9yaJ","executionInfo":{"status":"ok","timestamp":1711007023734,"user_tz":-540,"elapsed":5384,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"1c009177-b5d1-4318-a4df-e61d81cee8f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting grandalf\n","  Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m41.0/41.8 kB\u001b[0m \u001b[31m922.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m760.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from grandalf) (3.1.2)\n","Installing collected packages: grandalf\n","Successfully installed grandalf-0.8\n"]}]},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain.vectorstores import FAISS\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings"],"metadata":{"id":"vYHg4SXN9Z0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorstore = FAISS.from_texts(\n","    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",")\n","retriever = vectorstore.as_retriever()\n","\n","template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","model = ChatOpenAI()"],"metadata":{"id":"3fgNffik9cXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | model\n","    | StrOutputParser()\n",")"],"metadata":{"id":"rzj0--Z-9enZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 그래프 얻기"],"metadata":{"id":"d3nhSGgB9hYS"}},{"cell_type":"code","source":["# 실행 가능한 그래프를 얻는다.\n","chain.get_graph()"],"metadata":{"id":"6dvGezLE9gUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 그래프 인쇄"],"metadata":{"id":"6BtBfX-E9uWR"}},{"cell_type":"code","source":["chain.get_graph().print_ascii()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXP8QEg_9qrm","executionInfo":{"status":"ok","timestamp":1711007037816,"user_tz":-540,"elapsed":338,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"522815f9-516d-47a0-9a43-678f1c9345cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           +---------------------------------+         \n","           | Parallel<context,question>Input |         \n","           +---------------------------------+         \n","                    **               **                \n","                 ***                   ***             \n","               **                         **           \n","+----------------------+              +-------------+  \n","| VectorStoreRetriever |              | Passthrough |  \n","+----------------------+              +-------------+  \n","                    **               **                \n","                      ***         ***                  \n","                         **     **                     \n","           +----------------------------------+        \n","           | Parallel<context,question>Output |        \n","           +----------------------------------+        \n","                             *                         \n","                             *                         \n","                             *                         \n","                  +--------------------+               \n","                  | ChatPromptTemplate |               \n","                  +--------------------+               \n","                             *                         \n","                             *                         \n","                             *                         \n","                      +------------+                   \n","                      | ChatOpenAI |                   \n","                      +------------+                   \n","                             *                         \n","                             *                         \n","                             *                         \n","                   +-----------------+                 \n","                   | StrOutputParser |                 \n","                   +-----------------+                 \n","                             *                         \n","                             *                         \n","                             *                         \n","                +-----------------------+              \n","                | StrOutputParserOutput |              \n","                +-----------------------+              \n"]}]},{"cell_type":"markdown","source":["## 프롬프트 받기\n","##### 체인에 프롬프트가 표시되도록 한다."],"metadata":{"id":"2IG7sqrM95dq"}},{"cell_type":"code","source":["chain.get_prompts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AnRPWQl49wTv","executionInfo":{"status":"ok","timestamp":1711007064872,"user_tz":-540,"elapsed":300,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"96427e65-33bc-4dcc-cc5c-4b2d8641b61e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])]"]},"metadata":{},"execution_count":106}]},{"cell_type":"markdown","source":["# 메시지 기록(메모리) 추가\n","##### RunnableWithMessageHistory를 통해 특정 유형의 체인에 메시지 기록을 추가할 수 있다."],"metadata":{"id":"n4vFPsS3-ABK"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai.chat_models import ChatOpenAI\n","\n","model = ChatOpenAI()\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n","        ),\n","        # 기록 저장 홀더\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","runnable = prompt | model"],"metadata":{"id":"0cYN-1Bq9-B-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 인 메모리\n","##### 채팅 기록이 메모리에 저장되는 예제를 보자"],"metadata":{"id":"1EO4e5Bj-wYJ"}},{"cell_type":"code","source":["from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","\n","store = {}\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]\n","\n","\n","with_message_history = RunnableWithMessageHistory(\n","    runnable,\n","    get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"history\",\n",")"],"metadata":{"id":"zn-TQE6D-vWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n","    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zrmnsmtd_ZtJ","executionInfo":{"status":"ok","timestamp":1711007507006,"user_tz":-540,"elapsed":887,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f6b5df6c-52d1-4da9-b7f1-c7568aa3ed9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Cosine is a trigonometric function that gives the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 33, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["# 대화 내용을 기억한다\n","with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"What?\"},\n","    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcSCQ2WG_pwd","executionInfo":{"status":"ok","timestamp":1711007531603,"user_tz":-540,"elapsed":1341,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"16793633-5c77-4c91-f1f3-228c832bdca3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Cosine is a mathematical function that relates the ratio of the adjacent side to the hypotenuse in a right triangle.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 101, 'total_tokens': 125}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["# 하지만 세션이 달라질 경우 기억하지 못한다\n","with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"What?\"},\n","    config={\"configurable\": {\"session_id\": \"def234\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iC0NncM4_rqb","executionInfo":{"status":"ok","timestamp":1711007539795,"user_tz":-540,"elapsed":728,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"0141f209-5258-4714-d381-d87093ae4559"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='I can help with math problems! Just let me know what you need assistance with.', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 30, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":113}]},{"cell_type":"markdown","source":["##### 메시지 기록을 추적하는 구성 매개변수를 사용자 정의로 구성할 수 있다."],"metadata":{"id":"7lORW_dBAHeC"}},{"cell_type":"code","source":["from langchain_core.runnables import ConfigurableFieldSpec\n","\n","store = {}\n","\n","\n","def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n","    if (user_id, conversation_id) not in store:\n","        store[(user_id, conversation_id)] = ChatMessageHistory()\n","    return store[(user_id, conversation_id)]\n","\n","\n","with_message_history = RunnableWithMessageHistory(\n","    runnable,\n","    get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"history\",\n","    history_factory_config=[\n","        ConfigurableFieldSpec(\n","            id=\"user_id\",\n","            annotation=str,\n","            name=\"User ID\",\n","            description=\"Unique identifier for the user.\",\n","            default=\"\",\n","            is_shared=True,\n","        ),\n","        ConfigurableFieldSpec(\n","            id=\"conversation_id\",\n","            annotation=str,\n","            name=\"Conversation ID\",\n","            description=\"Unique identifier for the conversation.\",\n","            default=\"\",\n","            is_shared=True,\n","        ),\n","    ],\n",")"],"metadata":{"id":"l_VdKirH_x39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"Hello\"},\n","    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ic-1w-YAK9x","executionInfo":{"status":"ok","timestamp":1711007648338,"user_tz":-540,"elapsed":1473,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"ee3664b2-d6a7-408c-b209-93fc385eb653"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Hi! How can I assist you today with math or any other questions you may have?', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 29, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":115}]},{"cell_type":"markdown","source":["## 다양한 서명(입력)\n","##### dict"],"metadata":{"id":"s-naKNJcAS4Z"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","from langchain_core.runnables import RunnableParallel\n","\n","chain = RunnableParallel({\"output_message\": ChatOpenAI()})\n","\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]\n","\n","\n","with_message_history = RunnableWithMessageHistory(\n","    chain,\n","    get_session_history,\n","    output_messages_key=\"output_message\",\n",")\n","\n","with_message_history.invoke(\n","    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],\n","    config={\"configurable\": {\"session_id\": \"baz\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6IARJM5AMEL","executionInfo":{"status":"ok","timestamp":1711007698629,"user_tz":-540,"elapsed":2288,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"07481845-3147-4734-b4a4-e543146e79d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'output_message': AIMessage(content='Simone de Beauvoir believed that human beings have free will and the ability to make choices and shape their own lives. She argued that individuals are not bound by pre-determined roles or social norms, and that they have the power to define themselves through their actions and choices. Beauvoir believed in the importance of personal responsibility and agency, and rejected the idea of determinism or predestination.', response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 17, 'total_tokens': 95}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})}"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["with_message_history.invoke(\n","    [HumanMessage(content=\"How did this compare to Sartre\")],\n","    config={\"configurable\": {\"session_id\": \"baz\"}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmbVImyZAYHQ","executionInfo":{"status":"ok","timestamp":1711007705436,"user_tz":-540,"elapsed":4622,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"49c381d1-7afe-4c1a-c4a5-17c9b1f68003"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'output_message': AIMessage(content='Simone de Beauvoir\\'s beliefs about free will were heavily influenced by existentialist philosophy, particularly the work of Jean-Paul Sartre. Both Beauvoir and Sartre believed in the concept of radical freedom, which means that individuals are ultimately responsible for their own actions and choices. They rejected the idea of determinism and believed that human beings have the capacity to create their own values and meaning in a world that lacks inherent purpose or meaning.\\n\\nHowever, there were some differences in their views on free will. Sartre emphasized the idea of \"bad faith,\" which refers to individuals denying their own freedom and responsibility by conforming to societal expectations or norms. Beauvoir, on the other hand, focused more on the ways in which social structures and gender roles can limit individuals\\' freedom and autonomy. She argued that women, in particular, face societal constraints that can restrict their ability to exercise their free will.\\n\\nOverall, both Beauvoir and Sartre believed in the importance of individual choice and agency, but Beauvoir\\'s perspective also included a critical analysis of the ways in which societal expectations and power dynamics can influence and constrain individuals\\' freedom.', response_metadata={'token_usage': {'completion_tokens': 230, 'prompt_tokens': 111, 'total_tokens': 341}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})}"]},"metadata":{},"execution_count":117}]},{"cell_type":"markdown","source":["##### 메시지 입력"],"metadata":{"id":"8p1JGEttAaZC"}},{"cell_type":"code","source":["RunnableWithMessageHistory(\n","    ChatOpenAI(),\n","    get_session_history,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhUIt7GyAZPT","executionInfo":{"status":"ok","timestamp":1711007712605,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"fabfecac-da31-4489-d5b1-a571da3a9ecd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n","| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ce21d381360>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ce21d433670>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7ce21d408ca0>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7ce21d386320>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["from operator import itemgetter\n","\n","RunnableWithMessageHistory(\n","    itemgetter(\"input_messages\") | ChatOpenAI(),\n","    get_session_history,\n","    input_messages_key=\"input_messages\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6RsPB5pAcJu","executionInfo":{"status":"ok","timestamp":1711007718697,"user_tz":-540,"elapsed":257,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"eeff9bd5-c34e-4e19-c6c1-dca0760bd1d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n","  input_messages: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n","}), config={'run_name': 'insert_history'})\n","| RunnableBinding(bound=RunnableLambda(itemgetter('input_messages'))\n","  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ce21d431360>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ce21d430e20>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x7ce21d40aef0>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x7ce21d386320>, input_messages_key='input_messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"]},"metadata":{},"execution_count":119}]},{"cell_type":"markdown","source":["## 영구적인 저장\n","##### 대화를 영구적으로 저장할려면 공간이 필요하다. redis를 사용해보자"],"metadata":{"id":"9lxNRpPsAeGo"}},{"cell_type":"code","source":["!pip install --upgrade --quiet redis"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_lu3m3ZAdqq","executionInfo":{"status":"ok","timestamp":1711007754115,"user_tz":-540,"elapsed":6126,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"ad006804-4186-43ed-a16e-7b13f0e74900"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/251.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/251.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m225.3/251.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.8/251.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_community.chat_message_histories import RedisChatMessageHistory\n","\n","\n","def get_message_history(session_id: str) -> RedisChatMessageHistory:\n","    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n","\n","\n","with_message_history = RunnableWithMessageHistory(\n","    runnable,\n","    get_message_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"history\",\n",")"],"metadata":{"id":"EW7BWQy9Apoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n","    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",")"],"metadata":{"id":"q_rm0uZuAuLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with_message_history.invoke(\n","    {\"ability\": \"math\", \"input\": \"What's its inverse\"},\n","    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",")"],"metadata":{"id":"KvZ0FNixAvKB"},"execution_count":null,"outputs":[]}]}