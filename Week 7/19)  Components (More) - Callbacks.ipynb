{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBTmF44Vs3qMIcs502WJ+h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 콜백\n","##### Langchain은 여러 단계에 연결할 수 있는 콜백 시스템을 제공한다.\n","##### 로깅, 모니터링, 스트리밍 및 기타 작업에 유용하다."],"metadata":{"id":"LvtnAdka5thm"}},{"cell_type":"markdown","source":["## 콜백 핸들러\n","##### CallbackHandlers는 구독할 수 있는 각 이벤트에 대한 메서드가 있는 CallbackHandler 인터페이스를 구현하는 개체이다.\n","##### CallbackManager는 이벤트가 트리거될 때 각 핸들러에서 적절한 메서드를 호출한다."],"metadata":{"id":"pKs9qZ6m6Fpn"}},{"cell_type":"markdown","source":["```python\n","class BaseCallbackHandler:\n","    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n","\n","    def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when LLM starts running.\"\"\"\n","\n","    def on_chat_model_start(\n","        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when Chat Model starts running.\"\"\"\n","\n","    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n","        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n","\n","    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n","        \"\"\"Run when LLM ends running.\"\"\"\n","\n","    def on_llm_error(\n","        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when LLM errors.\"\"\"\n","\n","    def on_chain_start(\n","        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when chain starts running.\"\"\"\n","\n","    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n","        \"\"\"Run when chain ends running.\"\"\"\n","\n","    def on_chain_error(\n","        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when chain errors.\"\"\"\n","\n","    def on_tool_start(\n","        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when tool starts running.\"\"\"\n","\n","    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:\n","        \"\"\"Run when tool ends running.\"\"\"\n","\n","    def on_tool_error(\n","        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when tool errors.\"\"\"\n","\n","    def on_text(self, text: str, **kwargs: Any) -> Any:\n","        \"\"\"Run on arbitrary text.\"\"\"\n","\n","    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n","        \"\"\"Run on agent action.\"\"\"\n","\n","    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n","        \"\"\"Run on agent end.\"\"\"\n","```"],"metadata":{"id":"eausiiVt5tSp"}},{"cell_type":"markdown","source":["## 콜백 핸들러 예제\n","##### 가장 기본적인 핸들러인 StdOutCallbackHandler(모든 이벤트를 기록) 사용해보자.\n","##### verbose 가 True 인 경우 StdOutCallbackHandler를 명시적으로 전달하지 않아도 호출된다."],"metadata":{"id":"CPE59BUX6_dl"}},{"cell_type":"markdown","source":["```python\n","from langchain_core.callbacks import StdOutCallbackHandler\n","from langchain.chains import LLMChain\n","from langchain_openai import OpenAI\n","from langchain_core.prompts import PromptTemplate\n","\n","handler = StdOutCallbackHandler()\n","llm = OpenAI()\n","prompt = PromptTemplate.from_template(\"1 + {number} = \")\n","\n","# 생성자 콜백\n","chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n","chain.invoke({\"number\":2})\n","\n","# verbose flag 사용\n","chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n","chain.invoke({\"number\":2})\n","\n","# 요청 콜백\n","chain = LLMChain(llm=llm, prompt=prompt)\n","chain.invoke({\"number\":2}, {\"callbacks\":[handler]})\n","```"],"metadata":{"id":"aRtgA76BFCbt"}},{"cell_type":"markdown","source":["\n","\n","*   생성자 콜백 : 생성자에 정의되는데, 이 경우 콜백은 해당 객체에 대한 모든 호출에 사용되며 해당 객체로만 범위가 지정된다. - 단일 요청이 아닌 전체 체인에 해당하는 로깅, 모니터링에 주로 사용\n","*   요청 콜백 : 요청을 발행하는 데 사용되는 \"invoke\" 메소드에 정의된다. 이 경우 콜백은 해당 특정 요청과 여기에 포함된 모든 하위 요청에만 사용된다. - 단일 요청의 스트리밍에 주로 사용\n","\n"],"metadata":{"id":"JO86iY6dFMLr"}},{"cell_type":"markdown","source":["## 비동기 콜백\n","##### 비동기 API를 사용하려는 경우 AsyncCallbackHandler 를 사용하는 것이 권장된다.\n","##### 비동기 방법을 사용하는 동안 동기화를 사용하면 CallbackHandler가 사용되지만, 내부적으로 문제를 일으킬 수 있는 호출이 수행된다. (run_in_executor)"],"metadata":{"id":"b0fq1AYyFrcV"}},{"cell_type":"markdown","source":["```python\n","import asyncio\n","from typing import Any, Dict, List\n","\n","from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n","from langchain_core.messages import HumanMessage\n","from langchain_core.outputs import LLMResult\n","from langchain_openai import ChatOpenAI\n","\n","\n","class MyCustomSyncHandler(BaseCallbackHandler):\n","    def on_llm_new_token(self, token: str, **kwargs) -> None:\n","        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n","\n","\n","class MyCustomAsyncHandler(AsyncCallbackHandler):\n","    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n","\n","    async def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when chain starts running.\"\"\"\n","        print(\"zzzz....\")\n","        await asyncio.sleep(0.3)\n","        class_name = serialized[\"name\"]\n","        print(\"Hi! I just woke up. Your llm is starting\")\n","\n","    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n","        \"\"\"Run when chain ends running.\"\"\"\n","        print(\"zzzz....\")\n","        await asyncio.sleep(0.3)\n","        print(\"Hi! I just woke up. Your llm is ending\")\n","\n","# 스트리밍을 사용하기 위해, chatmodel 생성자에 streaming=True 옵션을 줘야한다.\n","# 추가적으로 사용자 정의 핸들러를 수행해야 한다.\n","chat = ChatOpenAI(\n","    max_tokens=25,\n","    streaming=True,\n","    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],\n",")\n","\n","await chat.agenerate([[HumanMessage(content=\"Tell me a joke\")]])\n","```"],"metadata":{"id":"I_Q9rOLCGeng"}},{"cell_type":"markdown","source":["## 사용자 정의 콜백 핸들러\n","##### 사용자 정의 콜백 핸들러를 생성하려면 콜백 핸들러가 처리할 이벤트와 이벤트가 트리거될 때 콜백 핸들러가 수행할 작업을 결정해야 한다.\n","##### 그 다음 콜백 핸들러를 생성자 콜백이나 요청 콜백으로 객체에 연결하면 된다."],"metadata":{"id":"B87YRs0gGwta"}},{"cell_type":"markdown","source":["```python\n","from langchain_core.callbacks import BaseCallbackHandler\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","\n","class MyCustomHandler(BaseCallbackHandler):\n","    def on_llm_new_token(self, token: str, **kwargs) -> None:\n","        print(f\"My custom handler, token: {token}\")\n","\n","\n","prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])\n","\n","model = ChatOpenAI(streaming=True, callbacks=[MyCustomHandler()])\n","\n","chain = prompt | model\n","\n","response = chain.invoke({\"animal\": \"bears\"})\n","```"],"metadata":{"id":"AFyJCXX0HQ2Z"}},{"cell_type":"markdown","source":["## 파일 로깅\n","##### Langchain은 파일에 로그를 쓸 수 있는 FileCallbackHandler 기능을 제공한다.\n","##### StdOutCallbackHandler와 유사하지만 로그를 표준 출력으로 출력하는 대신 로그를 파일에 쓴다."],"metadata":{"id":"NFj-tLXVHTz6"}},{"cell_type":"markdown","source":["```python\n","from langchain_core.callbacks import FileCallbackHandler, StdOutCallbackHandler\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","from loguru import logger\n","\n","logfile = \"output.log\"\n","\n","# 핸들러의 다른 출력을 기록한다.\n","logger.add(logfile, colorize=True, enqueue=True)\n","handler_1 = FileCallbackHandler(logfile)\n","handler_2 = StdOutCallbackHandler()\n","\n","prompt = PromptTemplate.from_template(\"1 + {number} = \")\n","model = OpenAI()\n","\n","# verbose flag에 따라, 표준 출력과 파일에 저장할 수도 있고, 파일에만 저장할 수 있다.\n","chain = prompt | model\n","\n","response = chain.invoke({\"number\": 2}, {\"callbacks\": [handler_1, handler_2]})\n","logger.info(response)\n","```"],"metadata":{"id":"19Ga46ZTH-4p"}},{"cell_type":"markdown","source":["##### 내용 확인 가능\n","```python\n","%pip install --upgrade --quiet  ansi2html > /dev/null\n","\n","from ansi2html import Ansi2HTMLConverter\n","from IPython.display import HTML, display\n","\n","with open(\"output.log\", \"r\") as f:\n","    content = f.read()\n","\n","conv = Ansi2HTMLConverter()\n","html = conv.convert(content, full=True)\n","\n","display(HTML(html))\n","```"],"metadata":{"id":"_gDP_2XsIUsZ"}},{"cell_type":"markdown","source":["## 다중 콜백 핸들러\n","##### 이전까지는 \"callbacks=\"를 사용하여 객체를 생성할 때 콜백 핸들러를 전달했는데, 해당 경우 콜백의 범위는 해당 특정 객체로 지정된다.\n","##### 실행할 때 arg 키워드를 CallbackHandlers 사용하여 전달하면 callbacks 실행과 관련된 모든 중첩 개체에서 해당 콜백이 실행된다.\n","##### 예로, 핸들러가 Agent를 통해 전달되면 에이전트와 관련된 모든 콜백 및 에이전트 실행과 관련된 모든 개체(Tools, LLMChain, LLM)에 사용된다.\n","##### 각각 개별 중첩 개체에 핸들러를 수동으로 연결할 필요가 없다!"],"metadata":{"id":"6itnSKtvIdy1"}},{"cell_type":"markdown","source":["```python\n","from typing import Any, Dict, List, Union\n","\n","from langchain.agents import AgentType, initialize_agent, load_tools\n","from langchain.callbacks.base import BaseCallbackHandler\n","from langchain_core.agents import AgentAction\n","from langchain_openai import OpenAI\n","\n","\n","# 첫번째로 사용자 정의 콜백 핸들러를 구현한다.\n","class MyCustomHandlerOne(BaseCallbackHandler):\n","    def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> Any:\n","        print(f\"on_llm_start {serialized['name']}\")\n","\n","    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n","        print(f\"on_new_token {token}\")\n","\n","    def on_llm_error(\n","        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n","    ) -> Any:\n","        \"\"\"Run when LLM errors.\"\"\"\n","\n","    def on_chain_start(\n","        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n","    ) -> Any:\n","        print(f\"on_chain_start {serialized['name']}\")\n","\n","    def on_tool_start(\n","        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n","    ) -> Any:\n","        print(f\"on_tool_start {serialized['name']}\")\n","\n","    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n","        print(f\"on_agent_action {action}\")\n","\n","\n","class MyCustomHandlerTwo(BaseCallbackHandler):\n","    def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> Any:\n","        print(f\"on_llm_start (I'm the second handler!!) {serialized['name']}\")\n","\n","\n","# 핸들러들을 초기화한다.\n","handler1 = MyCustomHandlerOne()\n","handler2 = MyCustomHandlerTwo()\n","\n","# agent를 정의한다.\n","llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])\n","tools = load_tools([\"llm-math\"], llm=llm)\n","agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n","\n","# agent 실행한다.\n","agent.run(\"What is 2 raised to the 0.235 power?\", callbacks=[handler1])\n","```"],"metadata":{"id":"VJfWbEIVJt4C"}},{"cell_type":"markdown","source":["## 태그\n","##### call() / run() / apply() 메소드 tags에 인수를 전달하여 콜백에 태그를 추가할 수 있다.\n","##### 예로, 특정 LLMChain에 대한 모든 요청을 기록하려는 경우 태그를 추가한 다음 해당 태그를 기준으로 로그를 필터링할 수 있다. (생성자와 요청 콜백 모두 태그 전달 가능)\n","##### 이런 태그는 \"start\" 콜백 메소드의 인수로 전달된다."],"metadata":{"id":"ld2lLDEfLH2M"}},{"cell_type":"markdown","source":["## 토큰 계산\n","##### Langchain은 토큰을 계산할 수 있는 컨텍스트 관리자를 제공한다."],"metadata":{"id":"1eS4dOLQL64i"}},{"cell_type":"markdown","source":["```python\n","import asyncio\n","\n","from langchain_community.callbacks import get_openai_callback\n","from langchain_openai import OpenAI\n","\n","llm = OpenAI(temperature=0)\n","with get_openai_callback() as cb:\n","    llm.invoke(\"What is the square root of 4?\")\n","\n","total_tokens = cb.total_tokens\n","assert total_tokens > 0\n","\n","with get_openai_callback() as cb:\n","    llm.invoke(\"What is the square root of 4?\")\n","    llm.invoke(\"What is the square root of 4?\")\n","\n","assert cb.total_tokens == total_tokens * 2\n","\n","# 컨텍스트 관리자 내에서 동시 실행을 할 수 있다.\n","with get_openai_callback() as cb:\n","    await asyncio.gather(\n","        *[llm.agenerate([\"What is the square root of 4?\"]) for _ in range(3)]\n","    )\n","\n","assert cb.total_tokens == total_tokens * 3\n","\n","# 컨텍스트 관리자의 동시성 안정성 가짐\n","task = asyncio.create_task(llm.agenerate([\"What is the square root of 4?\"]))\n","with get_openai_callback() as cb:\n","    await llm.agenerate([\"What is the square root of 4?\"])\n","\n","await task\n","assert cb.total_tokens == total_tokens\n","```"],"metadata":{"id":"1Op0G03DMBXF"}}]}