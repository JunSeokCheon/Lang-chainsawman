{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGQlnb7h/fl3VYVlQ7Y5fS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"cjdnkU89oXzF","executionInfo":{"status":"ok","timestamp":1711963566436,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"outputs":[],"source":["import os\n","os.environ['OPENAI_API_KEY'] = \"EXAMPLES\""]},{"cell_type":"markdown","source":["# 채팅 모델\n","##### 채팅 모델은 채팅 메시지를 입력으로 하고 체팅 메시지를 출력으로 반환하는 언어 모델이다."],"metadata":{"id":"ERXQ6Kik_fo5"}},{"cell_type":"markdown","source":["## QuickStart\n","##### 채팅 모델은 내부적으로 언어 모델을 사용하지만 사용하는 인터페이스가 약간 다르다.\n","##### 텍스트 입력/출력 API를 사용하는 대신 채팅 메시지가 입력 및 출력인 인터페이스를 사용한다."],"metadata":{"id":"TPQCAc8BBFzf"}},{"cell_type":"code","source":["!pip install -qU langchain-openai"],"metadata":{"id":"4__UHUf8-sCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"],"metadata":{"id":"k_zj9ZkUBtj5","executionInfo":{"status":"ok","timestamp":1711964385145,"user_tz":-540,"elapsed":4034,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage, SystemMessage\n","\n","messages = [\n","    SystemMessage(content=\"You're a helpful assistant\"),\n","    HumanMessage(content=\"What is the purpose of model regularization?\"),\n","]"],"metadata":{"id":"QhXibf8dB10f","executionInfo":{"status":"ok","timestamp":1711964402112,"user_tz":-540,"elapsed":2,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# invoke 출력\n","chat.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOvfPNE1B6uW","executionInfo":{"status":"ok","timestamp":1711964407662,"user_tz":-540,"elapsed":2506,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"b5c5e683-4927-49da-d2b9-1e64b7cc266c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, to the point where it performs poorly on new, unseen data. Regularization techniques introduce a penalty term to the model's loss function, discouraging overly complex models that may fit the training data too closely. By adding this penalty, regularization helps to generalize the model and improve its performance on unseen data, leading to better overall predictive accuracy.\", response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 24, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 스트리밍 출력\n","for chunk in chat.stream(messages):\n","    print(chunk.content, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUfIJHbRB7gB","executionInfo":{"status":"ok","timestamp":1711964416777,"user_tz":-540,"elapsed":2968,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"a8dbe39c-8be7-419a-b623-84feaccb7204"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model regularization is a technique used in machine learning to prevent overfitting, which occurs when a model learns the training data too well and performs poorly on new, unseen data. Regularization adds a penalty term to the model's loss function, discouraging the model from fitting the noise in the training data and instead focusing on the more prominent patterns.\n","\n","The purpose of model regularization is to improve the generalization ability of the model, ensuring that it performs well on unseen data by reducing the complexity of the model and preventing it from memorizing the training data. This helps to strike a balance between bias and variance, leading to better performance on new data and improving the model's ability to make accurate predictions."]}]},{"cell_type":"code","source":["# 배치 출력\n","chat.batch([messages])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNM8hRGFB9xh","executionInfo":{"status":"ok","timestamp":1711964443622,"user_tz":-540,"elapsed":4315,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"5fb10e6f-e230-4dba-9b0b-7b10ca3b8a08"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[AIMessage(content=\"Model regularization is a technique used in machine learning to prevent overfitting. Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations that are not representative of the underlying patterns in the data. Regularization helps to prevent overfitting by adding a penalty term to the model's loss function that discourages overly complex models.\\n\\nThe purpose of model regularization is to find a balance between fitting the training data well and generalizing to new, unseen data. By penalizing complex models, regularization encourages the model to prioritize simpler explanations that are more likely to generalize well to new data. This can lead to better performance on unseen data and improve the model's ability to make accurate predictions.\", response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 24, 'total_tokens': 166}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# 비동기 invoke 출력\n","await chat.ainvoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CSIsIbQCEBT","executionInfo":{"status":"ok","timestamp":1711964480459,"user_tz":-540,"elapsed":2947,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"ae91d38c-8607-42d3-f5e8-21616a96bfe2"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"Model regularization is a technique used in machine learning to prevent overfitting. Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the model's performance on new, unseen data. Regularization helps to address this issue by adding a penalty term to the model's loss function, which discourages the model from becoming too complex and helps it generalize better to new data.\\n\\nThere are different types of regularization techniques, such as L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization, each with its own way of penalizing the model's complexity. By incorporating regularization into the model training process, machine learning practitioners can improve the model's performance on unseen data and make it more robust and reliable.\", response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 24, 'total_tokens': 180}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 비동기 스트리밍 출력\n","async for chunk in chat.astream(messages):\n","    print(chunk.content, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLz9r_P8CNWM","executionInfo":{"status":"ok","timestamp":1711964492458,"user_tz":-540,"elapsed":3288,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"bd9e2e20-bfdc-4e1a-d090-1f61021ec30c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, including its noise and random fluctuations, which can lead to poor performance on unseen data. Regularization techniques add a penalty term to the model's loss function, discouraging overly complex models that may be prone to overfitting. By adding this penalty, regularization helps to find a balance between fitting the training data well and generalizing to new, unseen data. Some common regularization techniques include L1 regularization (Lasso), L2 regularization (Ridge), and dropout in neural networks."]}]},{"cell_type":"code","source":["# 비동기 스트리밍 로그 출력\n","async for chunk in chat.astream_log(messages):\n","    print(chunk)"],"metadata":{"id":"ivcsbx8oCQOZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 랭스미스"],"metadata":{"id":"ds1-wEZZCb5D"}},{"cell_type":"markdown","source":["```python\n","export LANGCHAIN_TRACING_V2=\"true\"\n","export LANGCHAIN_API_KEY=<your-api-key>\n","```"],"metadata":{"id":"jGL4VQ2PCik5"}},{"cell_type":"markdown","source":["## 함수 호출"],"metadata":{"id":"4qOaUV_LF37g"}},{"cell_type":"code","source":["!pip install -qU langchain-core langchain-openai"],"metadata":{"id":"71-tJvz2Cj1j","executionInfo":{"status":"ok","timestamp":1711965581995,"user_tz":-540,"elapsed":7528,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 바인딩\n","##### 다양한 함수형 객체의 형식을 지정하고 모델에 바인딩하는 도우미 메서드를 구현한다.\n","##### Pydantic 함수 스키마를 사용하여 이를 호출하는 다양한 모델을 얻는 방법을 살펴보자"],"metadata":{"id":"_LQblQccGfW0"}},{"cell_type":"code","source":["from langchain_core.pydantic_v1 import BaseModel, Field\n","\n","class Multiply(BaseModel):\n","    \"\"\"Multiply two integers together.\"\"\"\n","\n","    a: int = Field(..., description=\"First integer\")\n","    b: int = Field(..., description=\"Second integer\")"],"metadata":{"id":"u1RYHTmoGZMh","executionInfo":{"status":"ok","timestamp":1711965672928,"user_tz":-540,"elapsed":6,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!pip install -qU langchain-openai"],"metadata":{"id":"h4DRU8XKGxHU","executionInfo":{"status":"ok","timestamp":1711965687242,"user_tz":-540,"elapsed":8677,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"],"metadata":{"id":"zvxpNtQcG5Jn","executionInfo":{"status":"ok","timestamp":1711965706377,"user_tz":-540,"elapsed":258,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Multply 클래스를 모델에 바인딩\n","llm_with_tools = llm.bind_tools([Multiply])\n","llm_with_tools.invoke(\"what's 3 * 12\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2s2UhVGjGylY","executionInfo":{"status":"ok","timestamp":1711965727064,"user_tz":-540,"elapsed":824,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"dc54451e-1aa0-4cc2-be80-f1351ef4afd9"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_u81W8Ei0yTp4uMA8OP3xE3a5', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 62, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'tool_calls', 'logprobs': None})"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\n","\n","tool_chain = llm_with_tools | JsonOutputToolsParser()\n","tool_chain.invoke(\"what's 3 * 12\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZ87t7oXG-Nt","executionInfo":{"status":"ok","timestamp":1711965734262,"user_tz":-540,"elapsed":1127,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"b9c39993-42a0-44b0-ab4b-6976c0a073e1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'type': 'Multiply', 'args': {'a': 3, 'b': 12}}]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n","\n","tool_chain = llm_with_tools | PydanticToolsParser(tools=[Multiply])\n","tool_chain.invoke(\"what's 3 * 12\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-o1Ipn49G_0U","executionInfo":{"status":"ok","timestamp":1711965741598,"user_tz":-540,"elapsed":801,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"210e6bc1-7e08-4722-97e9-ca59c902ce0b"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Multiply(a=3, b=12)]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["##### 모델이 도구를 사용하지 않는 경우 tool_choice=\"any\"\n","##### 도구를 강제로 사용하고 한 번만 사용하도록 하려면 tool_choice=\"도구 이름\" 설정한다."],"metadata":{"id":"Or2GdJdQHIcM"}},{"cell_type":"markdown","source":["### 함수 스키마\n","#### 파이썬"],"metadata":{"id":"olM-iqquHU-b"}},{"cell_type":"code","source":["import json\n","\n","from langchain_core.utils.function_calling import convert_to_openai_tool\n","\n","\n","def multiply(a: int, b: int) -> int:\n","    \"\"\"Multiply two integers together.\n","\n","    Args:\n","        a: First integer\n","        b: Second integer\n","    \"\"\"\n","    return a * b\n","\n","\n","print(json.dumps(convert_to_openai_tool(multiply), indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2jeyC_XHBxc","executionInfo":{"status":"ok","timestamp":1711965836476,"user_tz":-540,"elapsed":405,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"11cc3e83-cbd8-416c-f492-38d1d7f4ddea"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"multiply\",\n","    \"description\": \"Multiply two integers together.\",\n","    \"parameters\": {\n","      \"type\": \"object\",\n","      \"properties\": {\n","        \"a\": {\n","          \"type\": \"integer\",\n","          \"description\": \"First integer\"\n","        },\n","        \"b\": {\n","          \"type\": \"integer\",\n","          \"description\": \"Second integer\"\n","        }\n","      },\n","      \"required\": [\n","        \"a\",\n","        \"b\"\n","      ]\n","    }\n","  }\n","}\n"]}]},{"cell_type":"markdown","source":["#### Pydantic"],"metadata":{"id":"0h0SAoLCHaSs"}},{"cell_type":"code","source":["from langchain_core.pydantic_v1 import BaseModel, Field\n","\n","\n","class multiply(BaseModel):\n","    \"\"\"Multiply two integers together.\"\"\"\n","\n","    a: int = Field(..., description=\"First integer\")\n","    b: int = Field(..., description=\"Second integer\")\n","\n","\n","print(json.dumps(convert_to_openai_tool(multiply), indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ErgmYmHoHZAA","executionInfo":{"status":"ok","timestamp":1711965853457,"user_tz":-540,"elapsed":313,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d8cfe5c6-29a5-4a23-baa1-72d1d8af0f90"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"multiply\",\n","    \"description\": \"Multiply two integers together.\",\n","    \"parameters\": {\n","      \"type\": \"object\",\n","      \"properties\": {\n","        \"a\": {\n","          \"description\": \"First integer\",\n","          \"type\": \"integer\"\n","        },\n","        \"b\": {\n","          \"description\": \"Second integer\",\n","          \"type\": \"integer\"\n","        }\n","      },\n","      \"required\": [\n","        \"a\",\n","        \"b\"\n","      ]\n","    }\n","  }\n","}\n"]}]},{"cell_type":"markdown","source":["#### LangChain"],"metadata":{"id":"EZ3EAJraHd86"}},{"cell_type":"code","source":["from typing import Any, Type\n","\n","from langchain_core.tools import BaseTool\n","\n","\n","class MultiplySchema(BaseModel):\n","    \"\"\"Multiply tool schema.\"\"\"\n","\n","    a: int = Field(..., description=\"First integer\")\n","    b: int = Field(..., description=\"Second integer\")\n","\n","\n","class Multiply(BaseTool):\n","    args_schema: Type[BaseModel] = MultiplySchema\n","    name: str = \"multiply\"\n","    description: str = \"Multiply two integers together.\"\n","\n","    def _run(self, a: int, b: int, **kwargs: Any) -> Any:\n","        return a * b\n","\n","\n","# Note: we're passing in a Multiply object not the class itself.\n","print(json.dumps(convert_to_openai_tool(Multiply()), indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"306gZD9BHdIQ","executionInfo":{"status":"ok","timestamp":1711965866014,"user_tz":-540,"elapsed":280,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"44bf35f7-5718-4c44-fef7-702bd0273124"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"type\": \"function\",\n","  \"function\": {\n","    \"name\": \"multiply\",\n","    \"description\": \"Multiply two integers together.\",\n","    \"parameters\": {\n","      \"type\": \"object\",\n","      \"properties\": {\n","        \"a\": {\n","          \"description\": \"First integer\",\n","          \"type\": \"integer\"\n","        },\n","        \"b\": {\n","          \"description\": \"Second integer\",\n","          \"type\": \"integer\"\n","        }\n","      },\n","      \"required\": [\n","        \"a\",\n","        \"b\"\n","      ]\n","    }\n","  }\n","}\n"]}]},{"cell_type":"markdown","source":["## 캐싱\n","##### 채팅 모델을 위한 선택적 캐싱 레이어를 제공한다.\n","\n","\n","\n","1.   동일한 완료를 여러 번 요청하는 경우가 많을 경우 LLM 제공업체에 대한 API 호출 수를 줄여 비용 절약\n","2.   LLM 제공업체에 대한 API 호출 수를 줄여 애플리케이션 속도 증가\n","\n"],"metadata":{"id":"1azZQrYNHiNx"}},{"cell_type":"code","source":["!pip install langchain"],"metadata":{"id":"Wl2o5fqEH16z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"],"metadata":{"id":"Qf3x8uxSHgMt","executionInfo":{"status":"ok","timestamp":1711965943282,"user_tz":-540,"elapsed":332,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from langchain.globals import set_llm_cache"],"metadata":{"id":"ciJt4ohJHzEK","executionInfo":{"status":"ok","timestamp":1711965989961,"user_tz":-540,"elapsed":282,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### 메모리 캐싱"],"metadata":{"id":"d-fyUNZAH_g7"}},{"cell_type":"code","source":["%%time\n","from langchain.cache import InMemoryCache\n","\n","set_llm_cache(InMemoryCache())\n","\n","# The first time, it is not yet in cache, so it should take longer\n","llm.predict(\"Tell me a joke\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"39YxL6uqH0lN","executionInfo":{"status":"ok","timestamp":1711966006032,"user_tz":-540,"elapsed":1024,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"51ca5563-1f5e-49a3-bbf7-190a7442deea"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 288 ms, sys: 35.7 ms, total: 323 ms\n","Wall time: 1.02 s\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["%%time\n","# The second time it is, so it goes faster\n","llm.predict(\"Tell me a joke\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"_dih0iF0ICP8","executionInfo":{"status":"ok","timestamp":1711966013223,"user_tz":-540,"elapsed":11,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d987c142-f5f0-446c-ff87-eab6c2aeff50"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.35 ms, sys: 0 ns, total: 3.35 ms\n","Wall time: 3.21 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["### SQLite 캐싱"],"metadata":{"id":"P2vVCJ_9IFu_"}},{"cell_type":"code","source":["from langchain.cache import SQLiteCache\n","\n","set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"],"metadata":{"id":"oXBnFHT6IENg","executionInfo":{"status":"ok","timestamp":1711966034601,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["%%time\n","# The first time, it is not yet in cache, so it should take longer\n","llm.predict(\"Tell me a joke\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"WSmYdwepIJey","executionInfo":{"status":"ok","timestamp":1711966039667,"user_tz":-540,"elapsed":649,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"0bb8164f-e5a5-4746-9db2-50a1e942a1ff"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 40.2 ms, sys: 1.78 ms, total: 42 ms\n","Wall time: 656 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["%%time\n","# The second time it is, so it goes faster\n","llm.predict(\"Tell me a joke\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"320lZdVMIKg6","executionInfo":{"status":"ok","timestamp":1711966044852,"user_tz":-540,"elapsed":12,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"5a28dbab-9a02-4b04-a42b-0f19bc93fda0"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 6.52 ms, sys: 0 ns, total: 6.52 ms\n","Wall time: 7.09 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## 사용자 정의 채팅 모델\n","##### Langchain 추상화를 사용하여 사용자 정의 채팅 모델을 만들어보자"],"metadata":{"id":"l-9Ep0CRIO5H"}},{"cell_type":"markdown","source":["### 입력 및 출력\n","#### 메시지\n","\n","\n","*   SystemMessage : AI 동작을 준비하는 데 사용되며 일반적으로 일련의 입력 메시지 중 첫 번째로 전달\n","*   HumanMessage : 채팅 모델과 상호작용하는 사람의 메시지\n","*   AIMessage : 채팅 모델의 메시지를 나타냅니다. 이는 텍스트일 수도 있고 도구 호출 요청일 수도 있다.\n","*   FunctionMessage/ToolMessage : 도구 호출 결과를 모델에 다시 전달하기 위한 메시지\n","\n"],"metadata":{"id":"Ou0zA8lyIVjJ"}},{"cell_type":"code","source":["from langchain_core.messages import (\n","    AIMessage,\n","    BaseMessage,\n","    FunctionMessage,\n","    HumanMessage,\n","    SystemMessage,\n","    ToolMessage,\n",")"],"metadata":{"id":"D6wJ78TOIMAA","executionInfo":{"status":"ok","timestamp":1711966205915,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["### 스트리밍\n","##### 모든 채팅 메시지에 Chunk가 포함되어 있다."],"metadata":{"id":"9bE5xMnLI0BE"}},{"cell_type":"code","source":["from langchain_core.messages import (\n","    AIMessageChunk,\n","    FunctionMessageChunk,\n","    HumanMessageChunk,\n","    SystemMessageChunk,\n","    ToolMessageChunk,\n",")"],"metadata":{"id":"oMr4K1kMIzSf","executionInfo":{"status":"ok","timestamp":1711966231017,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHiNaLdCI5dG","executionInfo":{"status":"ok","timestamp":1711966238484,"user_tz":-540,"elapsed":276,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"39994ee4-2cce-420a-dce7-011a1107ed7e"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessageChunk(content='Hello World!')"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["### 사용자 정의 채팅 모델 - 기본 채팅 모델 생성"],"metadata":{"id":"LcwgCpDLJBgd"}},{"cell_type":"code","source":["from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n","\n","from langchain_core.callbacks import (\n","    AsyncCallbackManagerForLLMRun,\n","    CallbackManagerForLLMRun,\n",")\n","from langchain_core.language_models import BaseChatModel, SimpleChatModel\n","from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n","from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n","from langchain_core.runnables import run_in_executor\n","\n","\n","class CustomChatModelAdvanced(BaseChatModel):\n","    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n","\n","    When contributing an implementation to LangChain, carefully document\n","    the model including the initialization parameters, include\n","    an example of how to initialize the model and include any relevant\n","    links to the underlying models documentation or API.\n","\n","    Example:\n","\n","        .. code-block:: python\n","\n","            model = CustomChatModel(n=2)\n","            result = model.invoke([HumanMessage(content=\"hello\")])\n","            result = model.batch([[HumanMessage(content=\"hello\")],\n","                                 [HumanMessage(content=\"world\")]])\n","    \"\"\"\n","\n","    n: int\n","    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n","\n","    def _generate(\n","        self,\n","        messages: List[BaseMessage],\n","        stop: Optional[List[str]] = None,\n","        run_manager: Optional[CallbackManagerForLLMRun] = None,\n","        **kwargs: Any,\n","    ) -> ChatResult:\n","        \"\"\"Override the _generate method to implement the chat model logic.\n","\n","        This can be a call to an API, a call to a local model, or any other\n","        implementation that generates a response to the input prompt.\n","\n","        Args:\n","            messages: the prompt composed of a list of messages.\n","            stop: a list of strings on which the model should stop generating.\n","                  If generation stops due to a stop token, the stop token itself\n","                  SHOULD BE INCLUDED as part of the output. This is not enforced\n","                  across models right now, but it's a good practice to follow since\n","                  it makes it much easier to parse the output of the model\n","                  downstream and understand why generation stopped.\n","            run_manager: A run manager with callbacks for the LLM.\n","        \"\"\"\n","        last_message = messages[-1]\n","        tokens = last_message.content[: self.n]\n","        message = AIMessage(content=tokens)\n","        generation = ChatGeneration(message=message)\n","        return ChatResult(generations=[generation])\n","\n","    def _stream(\n","        self,\n","        messages: List[BaseMessage],\n","        stop: Optional[List[str]] = None,\n","        run_manager: Optional[CallbackManagerForLLMRun] = None,\n","        **kwargs: Any,\n","    ) -> Iterator[ChatGenerationChunk]:\n","        \"\"\"Stream the output of the model.\n","\n","        This method should be implemented if the model can generate output\n","        in a streaming fashion. If the model does not support streaming,\n","        do not implement it. In that case streaming requests will be automatically\n","        handled by the _generate method.\n","\n","        Args:\n","            messages: the prompt composed of a list of messages.\n","            stop: a list of strings on which the model should stop generating.\n","                  If generation stops due to a stop token, the stop token itself\n","                  SHOULD BE INCLUDED as part of the output. This is not enforced\n","                  across models right now, but it's a good practice to follow since\n","                  it makes it much easier to parse the output of the model\n","                  downstream and understand why generation stopped.\n","            run_manager: A run manager with callbacks for the LLM.\n","        \"\"\"\n","        last_message = messages[-1]\n","        tokens = last_message.content[: self.n]\n","\n","        for token in tokens:\n","            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n","\n","            if run_manager:\n","                run_manager.on_llm_new_token(token, chunk=chunk)\n","\n","            yield chunk\n","\n","    async def _astream(\n","        self,\n","        messages: List[BaseMessage],\n","        stop: Optional[List[str]] = None,\n","        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n","        **kwargs: Any,\n","    ) -> AsyncIterator[ChatGenerationChunk]:\n","        \"\"\"An async variant of astream.\n","\n","        If not provided, the default behavior is to delegate to the _generate method.\n","\n","        The implementation below instead will delegate to `_stream` and will\n","        kick it off in a separate thread.\n","\n","        If you're able to natively support async, then by all means do so!\n","        \"\"\"\n","        result = await run_in_executor(\n","            None,\n","            self._stream,\n","            messages,\n","            stop=stop,\n","            run_manager=run_manager.get_sync() if run_manager else None,\n","            **kwargs,\n","        )\n","        for chunk in result:\n","            yield chunk\n","\n","    @property\n","    def _llm_type(self) -> str:\n","        \"\"\"Get the type of language model used by this chat model.\"\"\"\n","        return \"echoing-chat-model-advanced\"\n","\n","    @property\n","    def _identifying_params(self) -> Dict[str, Any]:\n","        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n","        return {\"n\": self.n}"],"metadata":{"id":"EcAd022eI7Lv","executionInfo":{"status":"ok","timestamp":1711966282516,"user_tz":-540,"elapsed":307,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["model = CustomChatModelAdvanced(n=3)"],"metadata":{"id":"bWmAjlFiJF8O","executionInfo":{"status":"ok","timestamp":1711966287119,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["model.invoke(\n","    [\n","        HumanMessage(content=\"hello!\"),\n","        AIMessage(content=\"Hi there human!\"),\n","        HumanMessage(content=\"Meow!\"),\n","    ]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcK6rbRZJHGR","executionInfo":{"status":"ok","timestamp":1711966291034,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"3c6dbcfe-957c-4eba-83b3-8abdb1f62f9d"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Meo')"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["model.invoke(\"hello\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTKhAIJpJH_8","executionInfo":{"status":"ok","timestamp":1711966299203,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"85499419-1a25-42d8-87bf-96e3185bf36a"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='hel')"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["model.batch([\"hello\", \"goodbye\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YidZVUoJJ5I","executionInfo":{"status":"ok","timestamp":1711966301358,"user_tz":-540,"elapsed":319,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"5dceccdd-f3a1-4df8-9db9-b86305163eac"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[AIMessage(content='hel'), AIMessage(content='goo')]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["for chunk in model.stream(\"cat\"):\n","    print(chunk.content, end=\"|\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YjEiSedJKjL","executionInfo":{"status":"ok","timestamp":1711966304961,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"e9164f29-647e-46a8-edee-1021779e3252"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["c|a|t|"]}]},{"cell_type":"code","source":["async for chunk in model.astream(\"cat\"):\n","    print(chunk.content, end=\"|\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evDp3pX1JLgA","executionInfo":{"status":"ok","timestamp":1711966326975,"user_tz":-540,"elapsed":8,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"34bbc5d0-b8e2-4595-fd57-e9689a7d8fa1"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["c|a|t|"]}]},{"cell_type":"code","source":["async for event in model.astream_events(\"cat\", version=\"v1\"):\n","    print(event)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0qNn1KKJQkO","executionInfo":{"status":"ok","timestamp":1711966343540,"user_tz":-540,"elapsed":4,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"946d4574-8688-4870-dee1-ae4beb6657ee"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["{'event': 'on_chat_model_start', 'run_id': '59fb09a6-28be-41dc-b927-8f806d82a16b', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}\n","{'event': 'on_chat_model_stream', 'run_id': '59fb09a6-28be-41dc-b927-8f806d82a16b', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c')}}\n","{'event': 'on_chat_model_stream', 'run_id': '59fb09a6-28be-41dc-b927-8f806d82a16b', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a')}}\n","{'event': 'on_chat_model_stream', 'run_id': '59fb09a6-28be-41dc-b927-8f806d82a16b', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t')}}\n","{'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': '59fb09a6-28be-41dc-b927-8f806d82a16b', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat')}}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n","  warn_beta(\n"]}]},{"cell_type":"markdown","source":["## 로그 확률 반환"],"metadata":{"id":"2IOMnJEUJhWv"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","# logprobs = True 매개변수를 구성\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\").bind(logprobs=True)\n","\n","msg = llm.invoke((\"human\", \"how are you today\"))"],"metadata":{"id":"bB1hLXERJU48","executionInfo":{"status":"ok","timestamp":1711966435299,"user_tz":-540,"elapsed":1481,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["msg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGc3e4szJw0V","executionInfo":{"status":"ok","timestamp":1711966459991,"user_tz":-540,"elapsed":5,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"2311f6f9-10b9-46c7-c79b-a1b364ae9ec1"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. But I'm here and ready to help you with any questions or tasks you have! How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 16, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': {'content': [{'token': 'I', 'bytes': [73], 'logprob': -0.2322815, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.38429767, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.20297308, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.002020236, 'top_logprobs': []}, {'token': ' computer', 'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114], 'logprob': -0.053281885, 'top_logprobs': []}, {'token': ' program', 'bytes': [32, 112, 114, 111, 103, 114, 97, 109], 'logprob': -5.371606e-05, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.08828467, 'top_logprobs': []}, {'token': ' so', 'bytes': [32, 115, 111], 'logprob': -0.0014859393, 'top_logprobs': []}, {'token': ' I', 'bytes': [32, 73], 'logprob': -5.8961017e-05, 'top_logprobs': []}, {'token': ' don', 'bytes': [32, 100, 111, 110], 'logprob': -0.0013445196, 'top_logprobs': []}, {'token': \"'t\", 'bytes': [39, 116], 'logprob': -1.3856493e-06, 'top_logprobs': []}, {'token': ' have', 'bytes': [32, 104, 97, 118, 101], 'logprob': -0.00423107, 'top_logprobs': []}, {'token': ' feelings', 'bytes': [32, 102, 101, 101, 108, 105, 110, 103, 115], 'logprob': -0.027036315, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.17860228, 'top_logprobs': []}, {'token': ' emotions', 'bytes': [32, 101, 109, 111, 116, 105, 111, 110, 115], 'logprob': -0.013936766, 'top_logprobs': []}, {'token': ' like', 'bytes': [32, 108, 105, 107, 101], 'logprob': -1.0455476, 'top_logprobs': []}, {'token': ' humans', 'bytes': [32, 104, 117, 109, 97, 110, 115], 'logprob': -0.12716076, 'top_logprobs': []}, {'token': ' do', 'bytes': [32, 100, 111], 'logprob': -0.0068064337, 'top_logprobs': []}, {'token': '.', 'bytes': [46], 'logprob': -0.0055798604, 'top_logprobs': []}, {'token': ' But', 'bytes': [32, 66, 117, 116], 'logprob': -0.14376953, 'top_logprobs': []}, {'token': ' I', 'bytes': [32, 73], 'logprob': -0.49374267, 'top_logprobs': []}, {'token': \"'m\", 'bytes': [39, 109], 'logprob': -0.006694646, 'top_logprobs': []}, {'token': ' here', 'bytes': [32, 104, 101, 114, 101], 'logprob': -0.0055617173, 'top_logprobs': []}, {'token': ' and', 'bytes': [32, 97, 110, 100], 'logprob': -0.3326763, 'top_logprobs': []}, {'token': ' ready', 'bytes': [32, 114, 101, 97, 100, 121], 'logprob': -0.001294047, 'top_logprobs': []}, {'token': ' to', 'bytes': [32, 116, 111], 'logprob': -4.3202e-07, 'top_logprobs': []}, {'token': ' help', 'bytes': [32, 104, 101, 108, 112], 'logprob': -0.94983274, 'top_logprobs': []}, {'token': ' you', 'bytes': [32, 121, 111, 117], 'logprob': -0.08451186, 'top_logprobs': []}, {'token': ' with', 'bytes': [32, 119, 105, 116, 104], 'logprob': -0.0010386447, 'top_logprobs': []}, {'token': ' any', 'bytes': [32, 97, 110, 121], 'logprob': -0.83811694, 'top_logprobs': []}, {'token': ' questions', 'bytes': [32, 113, 117, 101, 115, 116, 105, 111, 110, 115], 'logprob': -7.338466e-05, 'top_logprobs': []}, {'token': ' or', 'bytes': [32, 111, 114], 'logprob': -0.04286764, 'top_logprobs': []}, {'token': ' tasks', 'bytes': [32, 116, 97, 115, 107, 115], 'logprob': -0.07686035, 'top_logprobs': []}, {'token': ' you', 'bytes': [32, 121, 111, 117], 'logprob': -5.7172965e-05, 'top_logprobs': []}, {'token': ' have', 'bytes': [32, 104, 97, 118, 101], 'logprob': -0.07181533, 'top_logprobs': []}, {'token': '!', 'bytes': [33], 'logprob': -0.7012628, 'top_logprobs': []}, {'token': ' How', 'bytes': [32, 72, 111, 119], 'logprob': -0.36582834, 'top_logprobs': []}, {'token': ' can', 'bytes': [32, 99, 97, 110], 'logprob': -0.0004992975, 'top_logprobs': []}, {'token': ' I', 'bytes': [32, 73], 'logprob': -1.2664457e-06, 'top_logprobs': []}, {'token': ' assist', 'bytes': [32, 97, 115, 115, 105, 115, 116], 'logprob': -2.7729659e-05, 'top_logprobs': []}, {'token': ' you', 'bytes': [32, 121, 111, 117], 'logprob': -2.1008714e-06, 'top_logprobs': []}, {'token': ' today', 'bytes': [32, 116, 111, 100, 97, 121], 'logprob': -0.00024072826, 'top_logprobs': []}, {'token': '?', 'bytes': [63], 'logprob': -2.2603901e-05, 'top_logprobs': []}]}})"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["msg.response_metadata[\"logprobs\"][\"content\"][:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPRi9newJq75","executionInfo":{"status":"ok","timestamp":1711966444986,"user_tz":-540,"elapsed":3,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"3fb1620b-702b-4fe4-cf1b-703a26f57cc5"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'token': 'I', 'bytes': [73], 'logprob': -0.2322815, 'top_logprobs': []},\n"," {'token': \"'m\",\n","  'bytes': [39, 109],\n","  'logprob': -0.38429767,\n","  'top_logprobs': []},\n"," {'token': ' just',\n","  'bytes': [32, 106, 117, 115, 116],\n","  'logprob': -0.20297308,\n","  'top_logprobs': []},\n"," {'token': ' a',\n","  'bytes': [32, 97],\n","  'logprob': -0.002020236,\n","  'top_logprobs': []},\n"," {'token': ' computer',\n","  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],\n","  'logprob': -0.053281885,\n","  'top_logprobs': []}]"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["ct = 0\n","full = None\n","for chunk in llm.stream((\"human\", \"how are you today\")):\n","    if ct < 5:\n","        full = chunk if full is None else full + chunk\n","        if \"logprobs\" in full.response_metadata:\n","            print(full.response_metadata[\"logprobs\"][\"content\"])\n","    else:\n","        break\n","    ct += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVEzhZuqJthn","executionInfo":{"status":"ok","timestamp":1711966456118,"user_tz":-540,"elapsed":840,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"d9707745-53a5-497f-c428-5bf4b08c89eb"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[{'token': 'As', 'bytes': [65, 115], 'logprob': -1.76512, 'top_logprobs': []}]\n","[{'token': 'As', 'bytes': [65, 115], 'logprob': -1.76512, 'top_logprobs': []}, {'token': ' an', 'bytes': [32, 97, 110], 'logprob': -0.032747388, 'top_logprobs': []}]\n","[{'token': 'As', 'bytes': [65, 115], 'logprob': -1.76512, 'top_logprobs': []}, {'token': ' an', 'bytes': [32, 97, 110], 'logprob': -0.032747388, 'top_logprobs': []}, {'token': ' AI', 'bytes': [32, 65, 73], 'logprob': -0.009283651, 'top_logprobs': []}]\n","[{'token': 'As', 'bytes': [65, 115], 'logprob': -1.76512, 'top_logprobs': []}, {'token': ' an', 'bytes': [32, 97, 110], 'logprob': -0.032747388, 'top_logprobs': []}, {'token': ' AI', 'bytes': [32, 65, 73], 'logprob': -0.009283651, 'top_logprobs': []}, {'token': ',', 'bytes': [44], 'logprob': -0.054568645, 'top_logprobs': []}]\n"]}]},{"cell_type":"markdown","source":["## 스트리밍\n","##### 모든 ChatModel은 모든 메소드의 기본 구현과 함께 제공되는 Runnable 인터페이스를 구현"],"metadata":{"id":"FyFbwsLYJ1GK"}},{"cell_type":"code","source":["for chunk in llm.stream(\"Write me a song about goldfish on the moon\"):\n","    print(chunk.content, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alkD2NYrJwOI","executionInfo":{"status":"ok","timestamp":1711966541698,"user_tz":-540,"elapsed":7424,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f6aba683-f690-452e-85d4-f734cf9b0e5a"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Verse 1:\n","In a world where fish can fly\n","And the moon shines in the sky\n","There's a place where goldfish roam\n","In a universe of their own\n","\n","Chorus:\n","Goldfish on the moon\n","Swimming in the silver lagoon\n","A magical sight to see\n","In a place where dreams run free\n","\n","Verse 2:\n","They glide through the starry night\n","With their scales shimmering bright\n","Dancing in the cosmic sea\n","In a world of possibility\n","\n","Chorus:\n","Goldfish on the moon\n","Swimming in the silver lagoon\n","A magical sight to see\n","In a place where dreams run free\n","\n","Bridge:\n","They leap and twirl in zero gravity\n","In a celestial symphony\n","Their fins like wings, they soar and play\n","In a moonlit ballet\n","\n","Chorus:\n","Goldfish on the moon\n","Swimming in the silver lagoon\n","A magical sight to see\n","In a place where dreams run free\n","\n","Outro:\n","So if you ever feel alone\n","Just look up at the moon\n","And know that somewhere out there\n","There's goldfish swimming in the silver lagoon."]}]},{"cell_type":"markdown","source":["## 토큰 사용량 추적"],"metadata":{"id":"NW8XZQ8RKFYi"}},{"cell_type":"code","source":["from langchain.callbacks import get_openai_callback\n","from langchain_openai import ChatOpenAI"],"metadata":{"id":"ckl6-_GfKDdx","executionInfo":{"status":"ok","timestamp":1711966553458,"user_tz":-540,"elapsed":2,"user":{"displayName":"천준석","userId":"06369588489203998973"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# get_openai_callback를 사용해 토큰 사용량 추적\n","with get_openai_callback() as cb:\n","    result = llm.invoke(\"Tell me a joke\")\n","    print(cb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFiyvMoPKILU","executionInfo":{"status":"ok","timestamp":1711966559541,"user_tz":-540,"elapsed":754,"user":{"displayName":"천준석","userId":"06369588489203998973"}},"outputId":"f8f0886c-1df1-4dfa-9916-267f7c636049"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens Used: 28\n","\tPrompt Tokens: 11\n","\tCompletion Tokens: 17\n","Successful Requests: 1\n","Total Cost (USD): $3.1e-05\n"]}]},{"cell_type":"markdown","source":["##### 여러 단계가 포함된 체인이나 에이전트를 사용하는 경우 해당 단계를 모두 추적한다."],"metadata":{"id":"O50Q6edSKt66"}},{"cell_type":"code","source":["from langchain.agents import AgentType, initialize_agent, load_tools\n","from langchain_openai import OpenAI\n","\n","tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n","agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)"],"metadata":{"id":"2_iq5m4DKQze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with get_openai_callback() as cb:\n","    response = agent.run(\n","        \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n","    )\n","    print(f\"Total Tokens: {cb.total_tokens}\")\n","    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n","    print(f\"Completion Tokens: {cb.completion_tokens}\")\n","    print(f\"Total Cost (USD): ${cb.total_cost}\")"],"metadata":{"id":"Myr1KWhoKyiG"},"execution_count":null,"outputs":[]}]}