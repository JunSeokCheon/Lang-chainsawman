{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKxZiKzec1JXqPJdx85NDt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fallbacks(폴백)\n","##### 언어 모델로 작업할 때 속도 제한이나 가동 중지 시간과 같은 기본 API 문제가 발생할 수 있다.\n","##### 프로덕션 환경에서 Fallback은 긴급 상황을 대체할 수 있는 좋은 수단이다."],"metadata":{"id":"v0UnvBfWXqSK"}},{"cell_type":"markdown","source":["## LLM API 오류\n","##### LLM API에 대한 요청은 다양한 이유로 실패할 수 있다.(API 다운 or 비율 제한 등등)"],"metadata":{"id":"oQ15gSzLZcMt"}},{"cell_type":"markdown","source":["##### 먼저 OpenAI에서 RateLimitError가 발생하면 어떤 일이 일어나는지 확인해보자\n","```python\n","from unittest.mock import patch\n","\n","import httpx\n","from openai import RateLimitError\n","\n","request = httpx.Request(\"GET\", \"/\")\n","response = httpx.Response(200, request=request)\n","error = RateLimitError(\"rate limit\", response=response, body=\"\")\n","```\n","\n","```python\n","# RateLimits 반복을 피하기 위해 max_retries를 0으로 설정한다.\n","openai_llm = ChatOpenAI(max_retries=0)\n","anthropic_llm = ChatAnthropic()\n","llm = openai_llm.with_fallbacks([anthropic_llm])\n","\n","with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n","    try:\n","        print(openai_llm.invoke(\"Why did the chicken cross the road?\"))\n","    except RateLimitError:\n","        print(\"Hit error\")\n","\n","Hit error\n","```"],"metadata":{"id":"ZzC1vaB4ZcO5"}},{"cell_type":"markdown","source":["```python\n","# Anthropic 으로 대체하는 폴백을 해보자\n","with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n","    try:\n","        print(llm.invoke(\"Why did the chicken cross the road?\"))\n","    except RateLimitError:\n","        print(\"Hit error\")\n","\n","content=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False\n","```"],"metadata":{"id":"q14k2RB5ZcRI"}},{"cell_type":"markdown","source":["##### 또한, 일반 LLM 처럼 폴백이 포함된 LLM을 사용할 수 있다.\n","\n","```python\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You're a nice assistant who always includes a compliment in your response\",\n","        ),\n","        (\"human\", \"Why did the {animal} cross the road\"),\n","    ]\n",")\n","chain = prompt | llm\n","with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):\n","    try:\n","        print(chain.invoke({\"animal\": \"kangaroo\"}))\n","    except RateLimitError:\n","        print(\"Hit error\")\n","```"],"metadata":{"id":"UsYkcD2VZcTa"}},{"cell_type":"markdown","source":["## 시퀀스에 대한 대체\n","##### 시퀀스 자체인 시퀀스에 대한 대체를 만들 수 있다.\n","\n","```python\n","# ChatModel을 사용하여 체인을 만들고 문자열 출력 파서를 추가하여 동일한 출력이 되게 하자\n","from langchain_core.output_parsers import StrOutputParser\n","\n","chat_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You're a nice assistant who always includes a compliment in your response\",\n","        ),\n","        (\"human\", \"Why did the {animal} cross the road\"),\n","    ]\n",")\n","# 잘못된 모델 이름을 사용하여 오류가 발생하는 체인을 만들어보자\n","chat_model = ChatOpenAI(model=\"gpt-fake\")\n","bad_chain = chat_prompt | chat_model | StrOutputParser()\n","```"],"metadata":{"id":"S-jm4T1rZcVx"}},{"cell_type":"markdown","source":["```python\n","# 평범한 OpenAI 모델 체인을 만들어보자\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","\n","prompt_template = \"\"\"Instructions: You should always include a compliment in your response.\n","\n","Question: Why did the {animal} cross the road?\"\"\"\n","prompt = PromptTemplate.from_template(prompt_template)\n","llm = OpenAI()\n","good_chain = prompt | llm\n","```"],"metadata":{"id":"To8MWwD2XqUx"}},{"cell_type":"markdown","source":["```python\n","# 두 모델을 결합한 하나의 모델 체임을 만들 수 있다. bad_chain + good_chain\n","chain = bad_chain.with_fallbacks([good_chain])\n","chain.invoke({\"animal\": \"turtle\"})\n","\n","'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'\n","```"],"metadata":{"id":"edAAmPQgbCTz"}},{"cell_type":"markdown","source":["## 긴 입력\n","##### LLM의 가장 큰 제한 요소 중 하나는 컨텍스트 창이다.\n","##### 일반적으로 프롬프트를 LLM으로 보내기 전에 프롬프트의 길이를 계산하고 추적할 수 있지만, 이것이 어려운 상황이라면 컨텍스트 길이가 더 긴 모델로 대체할 수 있다."],"metadata":{"id":"a88myhbwbCV4"}},{"cell_type":"markdown","source":["```python\n","short_llm = ChatOpenAI()\n","long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n","llm = short_llm.with_fallbacks([long_llm])\n","\n","inputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 3000)\n","\n","try:\n","    print(short_llm.invoke(inputs))\n","except Exception as e:\n","    print(e)\n","\n","This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.\n","```"],"metadata":{"id":"EFutfXm3bCYA"}},{"cell_type":"markdown","source":["```python\n","try:\n","    print(llm.invoke(inputs))\n","except Exception as e:\n","    print(e)\n","\n","content='The next number in the sequence is two.' additional_kwargs={} example=False\n","```"],"metadata":{"id":"DELUhOPgbCaI"}},{"cell_type":"markdown","source":["## 더 나은 모델\n","##### 모델이 구문 분석에 실패하면 더 나은 모델을 사용하는 로직을 구성할 수 있다."],"metadata":{"id":"718yIHQlbCcg"}},{"cell_type":"markdown","source":["```python\n","from langchain.output_parsers import DatetimeOutputParser\n","\n","prompt = ChatPromptTemplate.from_template(\n","    \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\"\n",")\n","\n","# LLM + 출력 파서 수준에서 폴백을 수행한다. -> 해당 오류는 출력 파서에서 나타나기 때문에\n","openai_35 = ChatOpenAI() | DatetimeOutputParser()\n","openai_4 = ChatOpenAI(model=\"gpt-4\") | DatetimeOutputParser()\n","\n","only_35 = prompt | openai_35\n","fallback_4 = prompt | openai_35.with_fallbacks([openai_4])\n","\n","try:\n","    print(only_35.invoke({\"event\": \"the superbowl in 1994\"}))\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","\n","Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z\n","```"],"metadata":{"id":"t5ar6N_RbCew"}},{"cell_type":"markdown","source":["```python\n","try:\n","    print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"}))\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","\n","1994-01-30 15:30:00\n","```"],"metadata":{"id":"BD71lJptccap"}}]}